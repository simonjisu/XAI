{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(str(Path().home()/\"code\"/\"XAI\"/\"models\"))\n",
    "sys.path.append(str(Path().home()/\"code\"/\"XAI\"/\"models\"/\"relavance\"))\n",
    "sys.path.append(str(Path().home()/\"code\"/\"XAI\"/\"models\"/\"mnist\"))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from models.deconv.deconvnet import deconvMNIST\n",
    "from models.relavance.lrp import lrpMNIST\n",
    "from models.reshape import Reshape\n",
    "from models.relavance.layers import relConv2d, relLinear, relMaxPool2d, relReLU\n",
    "from models.mnist.MnistModels import MNISTmodel\n",
    "from models.mnist.MnistTrain import build_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LRP Method\n",
    "\n",
    "The Goal of LRP Method is calculate every relavance score for each neuron of each layer.\n",
    "\n",
    "$\\begin{aligned} \n",
    "r_i^{(L)} &= \\begin{cases}S_i(x) & \\text{if unit }i \\text{ is the target unit of interest}\\\\ 0 & \\text{otherwise}\\end{cases}  \\\\ r_i^{(l)} &= \\sum_j \\dfrac{z_{ji}}{\\sum_{i’}(z_{ji’}+b_j)+ \\epsilon \\cdot sign(\\sum_{i’}(z_{ji’}+b_j))}r_j^{(l+1)}\\\\ &\\text{where } z_{ji} = w_{ji}^{(l+1, l)}x_{i}^{(l)}\n",
    "\\end{aligned} $\n",
    "\n",
    "* the weight fraction $\\dfrac{z_{ji}}{\\sum_{i’}(z_{ji’}+b_j)+ \\epsilon \\cdot sign(\\sum_{i’}(z_{ji’}+b_j))}$ means how much share of a input neuron $x_i$ is to calculate the value of the output neuron $x_j$\n",
    "* numerator means score for input neuron $i$ to output neuron $j$\n",
    "* denominator means the value of output neuron $j$\n",
    "\n",
    "### calculate with the matrix\n",
    "\n",
    "* input feature $(1, \\cdots, i, \\cdots N)$\n",
    "* output feature $(1, \\cdots, j, \\cdots M)$\n",
    "* actually, weight $(N, M)$ is transposed to $(M, N)$ in pytorch\n",
    "\n",
    "$\\begin{aligned} X^{(l+1)} &= \\begin{bmatrix}  x_1 & \\cdots  &x_M \\end{bmatrix}^T\\\\\n",
    "X^{(l)} &= \\begin{bmatrix}  x_1 & \\cdots  & x_N \\end{bmatrix}^T\\\\\n",
    "W^{(l+1, l)} &= \\begin{bmatrix} \n",
    "w_{11} & \\cdots & w_{1i} & \\cdots & w_{1N} \\\\ \n",
    "\\vdots & \\ddots & \\ddots & \\ddots & \\vdots \\\\ \n",
    "w_{j1} & \\ddots & w_{ji} & \\ddots & w_{jN} \\\\\n",
    "\\vdots & \\ddots & \\ddots & \\ddots & \\vdots \\\\\n",
    "w_{M1} & \\cdots & w_{Mi} & \\cdots & w_{MN}\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}$\n",
    "\n",
    "to see element-wise calculation ...\n",
    "\n",
    "$\\begin{aligned} \n",
    "R^{(l+1)} &= \\begin{bmatrix} r_1 \\\\ \\vdots \\\\ r_i \\\\ \\vdots \\\\ r_N\\end{bmatrix}^{(l+1)} \n",
    "= \\begin{bmatrix} \\sum_j^M a_{1j}r_{1j} \\\\ \\vdots \\\\ \\sum_j^M a_{ij}r_{ij} \\\\ \\vdots \\\\ \\sum_j^M a_{Nj}r_{Nj} \\end{bmatrix}^{(l)} = \\begin{bmatrix} a_{11}r_{11} + \\cdots + a_{1M}r_{1M} \\\\ \\vdots \\\\ a_{i1}r_{i1} + \\cdots + a_{iM}r_{iM} \\\\ \\vdots \\\\ a_{N1}r_{N1} + \\cdots + a_{NM}r_{NM} \\end{bmatrix}^{(l)} \n",
    "\\\\\n",
    "Z^{(l, l+1)} &= \\begin{bmatrix} \n",
    "z_{11} & \\cdots & z_{1j} & \\cdots & z_{1M} \\\\ \n",
    "\\vdots & \\ddots & \\ddots & \\ddots & \\vdots \\\\ \n",
    "z_{i1} & \\ddots & z_{ij} & \\ddots & z_{ij} \\\\\n",
    "\\vdots & \\ddots & \\ddots & \\ddots & \\vdots \\\\\n",
    "z_{N1} & \\cdots & z_{Nj} & \\cdots & z_{NM}\n",
    "\\end{bmatrix} = \\begin{bmatrix} \n",
    "w_{11}x_1^{(l)} & \\cdots & w_{1j}x_j^{(l)} & \\cdots & w_{1M}x_M^{(l)} \\\\ \n",
    "\\vdots & \\ddots & \\ddots & \\ddots & \\vdots \\\\ \n",
    "w_{i1}x_1^{(l)} & \\ddots & w_{ij}x_j^{(l)} & \\ddots & w_{ij}x_M^{(l)} \\\\\n",
    "\\vdots & \\ddots & \\ddots & \\ddots & \\vdots \\\\\n",
    "w_{N1}x_1^{(l)} & \\cdots & w_{Nj}x_j^{(l)} & \\cdots & w_{NM}x_M^{(l)}\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}$\n",
    "\n",
    "### 1st way\n",
    "\n",
    "to get $r_i^{(l+1)}$ where $z_{ji}^{(l+1)} = w_{ji}^{(l+1, 1)} x_i^{(l)}$ there are 4 step in Linear Layer\n",
    "\n",
    "$\\begin{aligned} \n",
    "(1) & Z^{(l, l+1)} = W^{(l, l+1)} \\times X^{(l+1)}\\\\\n",
    "(2) & S^{(l+1)} = X^{(l+1)} + \\epsilon \\cdot sign(X^{(l+1)}) \\\\\n",
    "(3) & A^{(l, l+1)} = \\dfrac{Z^{(l, l+1)}}{S^{(l+1)}} \\\\\n",
    "(4) & R^{(l)} = A^{(l, l+1)}R^{(l+1)}  \\\\\n",
    "\\end{aligned}$\n",
    "\n",
    "### 2nd way\n",
    "\n",
    "same calculation but different order, introducing at http://heatmapping.org/tutorial/\n",
    "\n",
    "$\\begin{aligned} \n",
    "(1) & S^{(l+1)} = X^{(l+1)} + \\epsilon \\cdot sign(X^{(l+1)}) \\\\\n",
    "(2) & E^{(l+1)} = \\dfrac{R^{(l+1)}}{S^{(l+1)}} \\\\\n",
    "(3) & C^{(l)} = W^{(l, l+1)} E^{(l+1)} \\\\\n",
    "(4) & R^{(l)} = X^{(l)} \\times C^{(l)}  \\\\\n",
    "\\end{aligned}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nn.Linear(3, 2)\n",
    "b = relLinear(a)\n",
    "x = torch.rand(5, 3)\n",
    "output = b(x)\n",
    "r = torch.zeros(5, 2).scatter(1, torch.LongTensor([[1], [0], [0], [1], [0]]), 1)\n",
    "r_next = b.relprop(r)\n",
    "r_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In convolutional layer to get $r_i^{(l+1)}$ there are 4 step in Conv Layer, but change step 3 computing gradient of conv. which can be replaced as Transposed convolutional layer(=fractionally strided convolutional layer)\n",
    "\n",
    "$\\begin{aligned} \n",
    "(1) & S^{(l+1)} = X^{(l+1)} + \\epsilon \\cdot sign(X^{(l+1)}) \\\\\n",
    "(2) & E^{(l+1)} = \\dfrac{R^{(l+1)}}{S^{(l+1)}} \\\\\n",
    "(3) & C^{(l)} = \\triangledown (\\sum S^{(l+1)} \\times E^{(l+1)}) \\\\\n",
    "(4) & R^{(l)} = X^{(l)} \\times C^{(l)}  \\\\\n",
    "\\end{aligned}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nn.Conv2d(1, 32, 3)\n",
    "b = relConv2d(a)\n",
    "x = torch.randn(2, 1, 28, 28)\n",
    "output = b(x)\n",
    "r = torch.relu(output)\n",
    "r_next = b.relprop(r)\n",
    "r_next.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maxpooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nn.MaxPool2d(2, return_indices=True)\n",
    "b = relMaxPool2d(a)\n",
    "x = torch.randn(2, 32, 26, 26)\n",
    "output, swtiches = b(x)\n",
    "r_next = b.relprop(output)\n",
    "r_next.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = Reshape()\n",
    "x = torch.rand(2, 1, 12, 12)\n",
    "output = rs(x)\n",
    "output.size(), rs.relprop(output).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "class XaiHook(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super(XaiHook, self).__init__()\n",
    "        \"\"\"\n",
    "        Hook Handler Module\n",
    "        \n",
    "        supported register `module` hooks\n",
    "        - Activations\n",
    "        - Linear\n",
    "        - Convd\n",
    "        \n",
    "        like RNN have to use `register_hook` to `torch.nn.Parameter` directly\n",
    "        \n",
    "        * Ref: https://pytorch.org/docs/master/nn.html#torch.nn.Module.register_backward_hook\n",
    "        [Warnings]\n",
    "        The current implementation will not have the presented behavior \n",
    "        for complex Module that perform many operations. In some failure cases, \n",
    "        `grad_input` and `grad_output` will only contain the gradients for a subset\n",
    "        of the inputs and outputs. For such `Module`, you should use \n",
    "        `torch.Tensor.register_hook()` directly on a specific input or \n",
    "        output to get the required gradients.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.module = module\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.module.zero_grad()\n",
    "\n",
    "    def register_hook(self, backward=False, hook_fn=None):\n",
    "        \"\"\"\n",
    "        defalut hook_function is save (module, input, output) to (m, i, o)\n",
    "        if you want to use hook function, change `hook_function` \n",
    "        if `hook_function` returns `None` then the original input or output \n",
    "        will be flow into next / previous layer, but you can return a modifed\n",
    "        output/gradient to change the original output/gradient.\n",
    "        for a Conv2d layer example\n",
    "        - forward: a `Tensor` type output\n",
    "        - backward: (gradient_input, weight, bias)\n",
    "        \n",
    "        \"\"\"\n",
    "        def default_hook_fn(m, i, o):\n",
    "            \"\"\"\n",
    "            forward\n",
    "             - m: module class\n",
    "             - i: forward input from previous layer\n",
    "             - o: forward output to next layer\n",
    "            backward\n",
    "             - m: module class\n",
    "             - i: gradient input to next layer (backward out)\n",
    "             - o: gradient output from previous layer (backward in)\n",
    "\n",
    "            args:\n",
    "             * i, o: tuple type\n",
    "            \"\"\"\n",
    "            self.m = m\n",
    "            self.i = i\n",
    "            self.o = o\n",
    "            \n",
    "        if hook_fn is None:\n",
    "            self.hook_fn = default_hook_fn\n",
    "        else:\n",
    "            self.hook_fn = hook_fn\n",
    "        if not backward:\n",
    "            self.hook = self.module.register_forward_hook(self.hook_fn)\n",
    "        else:\n",
    "            self.hook = self.module.register_backward_hook(self.hook_fn)\n",
    "            \n",
    "    def close(self):\n",
    "        self.hook.remove()\n",
    "\n",
    "\n",
    "class XaiBase(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XaiBase, self).__init__()\n",
    "        \"\"\"\n",
    "        - need to define XaiHook class to use\n",
    "        - defalut hook_function is save (module, input, output) to (m, i, o)\n",
    "          if you want to use hook function, change `hook_function` \n",
    "        \"\"\"\n",
    "        self._reset_maps()\n",
    "    \n",
    "    def _reset_maps(self):\n",
    "        self.maps = OrderedDict()\n",
    "        \n",
    "    def _save_maps(self, layer_name, x):\n",
    "        self.maps[layer_name] = x    \n",
    "        \n",
    "    def _register(self, hooks, backward=False, hook_fn=None):\n",
    "        \"\"\"\n",
    "        - need to define XaiHook class to use\n",
    "        - defalut hook_function is save (module, input, output) to (m, i, o)\n",
    "          if you want to use hook function, change `hook_function` \n",
    "        \"\"\"\n",
    "        if not isinstance(hooks, list):\n",
    "            hooks = [hooks]\n",
    "        for hook in hooks:\n",
    "            hook.register_hook(backward=backward, hook_fn=hook_fn)\n",
    "    \n",
    "    def _register_forward(self, hooks, hook_fn=None):\n",
    "        self._register(hooks, backward=False, hook_fn=hook_fn)\n",
    "        \n",
    "    def _register_backward(self, hooks, hook_fn=None):\n",
    "        self._register(hooks, backward=True, hook_fn=hook_fn)\n",
    "    \n",
    "    def _reset_hooks(self, hooks):\n",
    "        if not isinstance(hooks, list):\n",
    "            hooks = [hooks]\n",
    "        for hook in hooks:\n",
    "            hook.close()\n",
    "\n",
    "    def _return_indices(self, layers, on=True):\n",
    "        \"\"\"\n",
    "        support for cnn layer which have `nn.MaxPool2d`,\n",
    "        you can turn on/off pooling indices.\n",
    "        please define a forward function to use it in your model\n",
    "        '''\n",
    "        # in your model\n",
    "        def forward_switch(self, x):\n",
    "            switches = OrderedDict()\n",
    "            self.return_indices(on=True)\n",
    "            for idx, layer in enumerate(self.convs):\n",
    "                if isinstance(layer, nn.MaxPool2d):\n",
    "                    x, indices = layer(x)\n",
    "                    switches[idx] = indices\n",
    "                else:\n",
    "                    x = layer(x)\n",
    "            self.return_indices(on=False)\n",
    "            return x, switches\n",
    "        '''\n",
    "        \"\"\"\n",
    "        if on:\n",
    "            for layer in layers:\n",
    "                if isinstance(layer, nn.MaxPool2d):\n",
    "                    layer.return_indices = True\n",
    "        else:\n",
    "            for layer in layers:\n",
    "                if isinstance(layer, nn.MaxPool2d):\n",
    "                    layer.return_indices = False  \n",
    "                    \n",
    "                    \n",
    "class XaiModel(XaiBase):\n",
    "    def __init__(self, model):\n",
    "        super(XaiModel, self).__init__()\n",
    "        self.model = deepcopy(model)\n",
    "        self.model.cpu()\n",
    "        self.model.eval()\n",
    "        \n",
    "    def _one_hot(self, targets, module_name):\n",
    "        \"\"\"\n",
    "        one hot vectorize the target tensor for classification purpose.\n",
    "        the `module` with respect to `module_name` must have `out_features` attribution.\n",
    "        args:\n",
    "        - targets: torch.LongTensor, target classes that have size of mini-batch\n",
    "        - module_name: str, feature name for Fully-Connected Network or any Task-specific Network\n",
    "        return:\n",
    "        - one hot vector of targets\n",
    "        \"\"\"\n",
    "        assert isinstance(targets, torch.LongTensor), \"`targets` must be `torch.LongTensor` type\"\n",
    "        assert isinstance(module_name, str), \"`module_name` must be `str` type\"\n",
    "        modules = self.model._modules[module_name]\n",
    "        if isinstance(modules, nn.Sequential):\n",
    "            last_layer = modules[-1]\n",
    "        else:\n",
    "            last_layer = modules\n",
    "        try:\n",
    "            last_layer.out_features \n",
    "        except AttributeError as e:\n",
    "            is_linear = isinstance(last_layer, nn.Linear)\n",
    "            print(f\"last layer of module `{module_name}` doesn't have `out_features` attribute\")\n",
    "            print()\n",
    "            if not is_linear:\n",
    "                print(f\"type of the last layer is `{type(last_layer)}`\")\n",
    "                print(\"the last layer is not `torch.nn.linear.Linear` class\")\n",
    "                print(\"create `.out_featrues` attribution in the custom module\")\n",
    "                \n",
    "        target_size = last_layer.out_features\n",
    "        B = targets.size(0)\n",
    "        one_hot = torch.zeros((B, target_size))\n",
    "        one_hot.scatter_(1, targets.unsqueeze(1), 1.0)\n",
    "        return one_hot.to(targets.device)\n",
    "    \n",
    "    def _find_target_layer_idx(self, module_name, layer_names):\n",
    "        assert isinstance(layer_names, list) or isinstance(layer_names, tuple), \"use list for `layer_names`\"\n",
    "        layer_names = [l.lower() for l in layer_names]\n",
    "        idxes = defaultdict(list)\n",
    "        modules = self.model._modules[module_name]\n",
    "        assert isinstance(modules, nn.Sequential), \"use this function for `nn.Sequential` type modules\"\n",
    "        for idx, layer in modules.named_children():\n",
    "            l_name = type(layer).__name__.lower()\n",
    "            if l_name in layer_names:\n",
    "                idxes[l_name].append(int(idx))\n",
    "\n",
    "        return idxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for New LRP Layer\n",
    "\n",
    "### relLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relLinear(XaiHook):\n",
    "    def __init__(self, module, use_rho=False):\n",
    "        \"\"\"\n",
    "        forward\n",
    "        > input: (B, in_f)\n",
    "        > output: (B, out_f)\n",
    "        \"\"\"\n",
    "        super(relLinear, self).__init__(module)\n",
    "        self.use_rho = use_rho\n",
    "        self.register_hook(backward=False, hook_fn=self.f_hook)\n",
    "        self.register_hook(backward=True, hook_fn=self.b_hook)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.module(x)\n",
    "    \n",
    "    def f_hook(self, m, i, o):\n",
    "        \"\"\"\n",
    "        forward hook\n",
    "        i: (input,)\n",
    "        o: output\n",
    "        \"\"\"\n",
    "        self.input = i[0].clone().data\n",
    "        self.output = o.clone().data\n",
    "    \n",
    "    def b_hook(self, m, i, o):\n",
    "        \"\"\"\n",
    "        backward hook\n",
    "        i: (grad_bias, input, grad_weight.T) -> backward output\n",
    "        o: (output,) -> backward input\n",
    "        \n",
    "        ### implementation method 1\n",
    "        [Step 1]: (B, in_f, 1) * (1, in_f, out_f) = (B, in_f, out_f)\n",
    "        [Step 2]: (B, 1, out_f), do not multiply `torch.sign(self.output.unsqueeze(1))` \n",
    "                  that returns `nan` in tensor\n",
    "        [Step 3]: divide by s\n",
    "        [Step 4]: (B, in_f, out_f) x (B, out_f, 1) = (B, in_f)\n",
    "        ```\n",
    "        # Step 1\n",
    "        z = self.input.unsqueeze(-1) * self.rho(self.weight).transpose(0, 1).unsqueeze(0)\n",
    "        # Step 2\n",
    "        s = self.output.unsqueeze(1) + eps * torch.sign(self.output.unsqueeze(1))  \n",
    "        # Step 3\n",
    "        weight = z / s\n",
    "        # Step 4\n",
    "        r_next = torch.bmm(weight, r.unsqueeze(-1)).squeeze()\n",
    "        ```\n",
    "        ### implemetation method 2\n",
    "        # Step 1: (B, out_f), do not multiply `torch.sign(self.output)` that returns `nan` in tensor\n",
    "        # Step 2: (B, out_f) / (B, out_f) = (B, out_f)\n",
    "        # Step 3: (B, in_f, out_f) * (B, out_f, 1) = (B, in_f)\n",
    "        # Step 4: (B, in_f) x (B, in_f) = (B, in_f)\n",
    "        ```\n",
    "        # Step 1\n",
    "        s = self.output + eps\n",
    "        # Step 2\n",
    "        e = r / s\n",
    "        # Step 3\n",
    "        c = torch.bmm(w.transpose(0, 1).expand(e.size(0), self.in_features, self.out_features), \n",
    "                      e.unsqueeze(-1)).squeeze(-1)\n",
    "        # Step 4\n",
    "        r_next = self.input * c\n",
    "        ```\n",
    "        \"\"\"\n",
    "        grad_bias, _, grad_weight = i\n",
    "        r = o[0]\n",
    "        eps = 1e-6\n",
    "        w = self.rho(self.module.weight).data\n",
    "        # Step 1\n",
    "        s = self.output + eps\n",
    "        # Step 2\n",
    "        e = r / s\n",
    "        # Step 3\n",
    "        c = torch.bmm(w.transpose(0, 1).expand(e.size(0), \n",
    "                                               self.module.in_features, \n",
    "                                               self.module.out_features), \n",
    "                      e.unsqueeze(-1)).squeeze(-1)\n",
    "        # Step 4\n",
    "        r_next = self.input * c\n",
    "        assert r_next.size(1) == self.module.in_features, \"size of `r_next` is not correct\"\n",
    "        # for debugging\n",
    "        self.r = r  \n",
    "        self.r_next = r_next\n",
    "        return (grad_bias, r_next, grad_weight)\n",
    "        \n",
    "    def rho(self, w):\n",
    "        if self.use_rho:\n",
    "            return torch.clamp(w, min=0)\n",
    "        else:\n",
    "            return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hook(m, i, o):\n",
    "    print(m)\n",
    "    print(type(i))\n",
    "    for tensor in i:\n",
    "        print(tensor)\n",
    "    print(type(o))\n",
    "    print(o)\n",
    "    print()\n",
    "relu = nn.ReLU()\n",
    "# relu.register_backward_hook(hook)\n",
    "linear_p = nn.Linear(5, 4)\n",
    "# linear_p.weight.data = torch.FloatTensor([[1, 2, 3], [1, 2, 3]])\n",
    "# linear_p.bias.data = torch.FloatTensor([1, 2])\n",
    "\n",
    "a = relLinear(linear_p)\n",
    "# linear_p.register_backward_hook(hook)\n",
    "\n",
    "linear = nn.Linear(4, 3)\n",
    "# linear.weight.data = torch.FloatTensor([[2, 3]])\n",
    "# linear.bias.data = torch.FloatTensor([1])\n",
    "\n",
    "b = relLinear(linear)\n",
    "# linear.register_backward_hook(hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 5).requires_grad_(True)\n",
    "o1 = a(x)\n",
    "o2 = relu(o1)\n",
    "o3 = b(o2)\n",
    "o3.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3.backward(torch.FloatTensor([[1, 0, 0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if all equal returns True\n",
    "(x.grad == a.r_next).sum() == x.grad.view(-1).size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### relConv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relConv2d(XaiHook):\n",
    "    \"\"\"relConv2d\"\"\"\n",
    "    def __init__(self, module, use_rho=False):\n",
    "        \"\"\"\n",
    "        forward\n",
    "        > input: (B, C_in, H_in, W_in)\n",
    "        > output: (B, C_out, H_out, W_out)\n",
    "        backward\n",
    "        > lrp propagation with respect to previous input\n",
    "        \"\"\"\n",
    "        super(relConv2d, self).__init__(module)\n",
    "        self.use_rho = use_rho\n",
    "        self.register_hook(backward=False, hook_fn=self.f_hook)\n",
    "        self.register_hook(backward=True, hook_fn=self.b_hook)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.module(x)\n",
    "    \n",
    "    def f_hook(self, m, i, o):\n",
    "        \"\"\"\n",
    "        forward hook\n",
    "        i: (input,)\n",
    "        o: output\n",
    "        \"\"\"\n",
    "        self.input = i[0].clone().data\n",
    "        self.output = o.clone().data\n",
    "    \n",
    "    def b_hook(self, m, i, o):\n",
    "        \"\"\"\n",
    "        backward hook\n",
    "        i: (grad_input, grad_weight, gard_bias) -> backward output\n",
    "        o: (gard_output,) -> backward input\n",
    "        \n",
    "        ### implementation method \n",
    "        [Step 1]: (B, C_out, H_out, W_out), do not multiply `torch.sign(self.output)` \n",
    "                   that returns `nan` in tensor\n",
    "        [Step 2]: (B, C_out, H_out, W_out) / (B, C_out, H_out, W_out) = (B, C_out, H_out, W_out)\n",
    "        [Step 3]: (B, C_out, H_out, W_out) --> (B, C_in, H, W)\n",
    "                  same as `self.gradprop(s*e)` or `(s*e).backward(); c=self.input.grad`\n",
    "        [Stpe 4]: (B, C_in, H, W) x (B, C_in, H, W) = (B, C_in, H, W)\n",
    "        \n",
    "        ```\n",
    "        # Step 1\n",
    "        s = self.output + eps \n",
    "        # Step 2\n",
    "        e = r / s\n",
    "        # Step 3:\n",
    "        c = self.gradprop(e)\n",
    "        # Step 4\n",
    "        r_next = self.input * c\n",
    "        ```\n",
    "        \"\"\"\n",
    "        _, grad_weight, grad_bias = i\n",
    "        r = o[0]\n",
    "        eps = 1e-6\n",
    "        w = self.rho(self.module.weight)\n",
    "        # Step 1\n",
    "        s = self.output + eps \n",
    "        # Step 2\n",
    "        e = r / s\n",
    "        # Step 3:\n",
    "        c = self.gradprop(e)\n",
    "        # Step 4\n",
    "        r_next = self.input * c\n",
    "\n",
    "        # for debugging\n",
    "        self.r = r  \n",
    "        self.r_next = r_next\n",
    "        return (r_next, grad_weight, grad_bias)\n",
    "        \n",
    "    def rho(self, w):\n",
    "        if self.use_rho:\n",
    "            return torch.clamp(w, min=0)\n",
    "        else:\n",
    "            return w\n",
    "\n",
    "    def gradprop(self, x):\n",
    "        \"\"\"\n",
    "        `ConvTransposed2d` can be seen as the gradient of `Conv2d` with respect to its input.\n",
    "        \"\"\"\n",
    "        output_padding = self.cal_output_padding()\n",
    "        c = torch.nn.functional.conv_transpose2d(x, \n",
    "                                                 weight=self.module.weight, \n",
    "                                                 stride=self.module.stride, \n",
    "                                                 padding=self.module.padding, \n",
    "                                                 output_padding=output_padding)\n",
    "        return c        \n",
    "\n",
    "    def cal_output_padding(self):\n",
    "        \"\"\"\n",
    "        calculate output_padding size\n",
    "        - size of height or width: (X_in + 2P - K) / S + 1 = X_out\n",
    "        - output_padding = X_in - ((X_out - 1) * S + K - 2P)\n",
    "\n",
    "        * what is output_padding?\n",
    "        from PyTorch Document:\n",
    "        https://pytorch.org/docs/stable/nn.html#convtranspose2d\n",
    "\n",
    "        The padding argument effectively adds `dilation * (kernel_size - 1) - padding` amount of zero padding to \n",
    "        both sizes of the input. This is set so that when a `Conv2d` and a `ConvTranspose2d` are initialized with \n",
    "        same parameters, they are inverses of each other in regard to the input and output shapes. \n",
    "        However, when `stride > 1`, `Conv2d` maps multiple input shapes to the same output shape. \n",
    "        `output_padding` is provided to resolve this ambiguity by effectively increasing \n",
    "        the calculated output shape on one side. Note that output_padding is only used to find output shape, \n",
    "        but does not actually add zero-padding to output.\n",
    "        \"\"\"\n",
    "        H_in, W_in = self.input.size()[2:]\n",
    "        H_out, W_out = self.output.size()[2:]\n",
    "        S_h, S_w = self.module.stride\n",
    "        K_h, K_w = self.module.kernel_size\n",
    "        P_h, P_w = self.module.padding\n",
    "        H_output_padding = H_in - ((H_out - 1)*S_h + K_h - 2*P_h)\n",
    "        W_output_padding = W_in - ((W_out - 1)*S_w + K_w - 2*P_w)\n",
    "        return (H_output_padding, W_output_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hook(m, i, o):\n",
    "    print(m)\n",
    "    print(type(i))\n",
    "    for tensor in i:\n",
    "        print(tensor)\n",
    "    print(type(o))\n",
    "    print(o)\n",
    "    print()\n",
    "relu = nn.ReLU()\n",
    "# relu.register_backward_hook(hook)\n",
    "conv_p = nn.Conv2d(1, 3, 3)\n",
    "\n",
    "a = relConv2d(conv_p)\n",
    "# conv_p.register_backward_hook(hook)\n",
    "\n",
    "conv = nn.Conv2d(3, 5, 3)\n",
    "\n",
    "b = relConv2d(conv)\n",
    "# conv.register_backward_hook(hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 7, 7])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 1, 11, 11).requires_grad_(True)\n",
    "o1 = a(x)\n",
    "o2 = relu(o1)\n",
    "o3 = b(o2)\n",
    "o3.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if all equal returns True\n",
    "(x.grad == a.r_next).sum() == x.grad.view(-1).size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### relMaxPool2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relMaxPool2d(XaiHook):\n",
    "    \"\"\"relMaxPool2d\"\"\"\n",
    "    def __init__(self, module, use_rho=False):\n",
    "        \"\"\"\n",
    "        forward\n",
    "        > input: (B, C, H_in, W_in)\n",
    "        > output: (B, C, H_out, W_out)\n",
    "        backward\n",
    "        > lrp propagation with respect to previous input\n",
    "        \"\"\"\n",
    "        super(relMaxPool2d, self).__init__(module)\n",
    "        self.use_rho = use_rho\n",
    "        self.register_hook(backward=False, hook_fn=self.f_hook)\n",
    "        self.register_hook(backward=True, hook_fn=self.b_hook)\n",
    "            \n",
    "    def __call__(self, x):\n",
    "        return self.module(x)\n",
    "    \n",
    "    def f_hook(self, m, i, o):\n",
    "        \"\"\"\n",
    "        forward hook\n",
    "        i: (input,)\n",
    "        o: output\n",
    "        \n",
    "        save forward input and output data\n",
    "        \"\"\"\n",
    "        self.input = i[0].clone().data\n",
    "        self.output = o.clone().data\n",
    "        \n",
    "    def b_hook(self, m, i, o):\n",
    "        \"\"\"\n",
    "        backward hook\n",
    "        i: (grad_input,) -> backward output\n",
    "        o: (gard_output,) -> backward input\n",
    "        \n",
    "        ### implementation method \n",
    "        [Step 1]: (B, C, H_out, W_out), do not multiply `torch.sign(self.output)` \n",
    "                  that returns `nan` in tensor\n",
    "        [Step 2]: (B, C, H_out, W_out) / (B, C, H_out, W_out) = (B, C, H_out, W_out)\n",
    "        [Step 3]: (B, C, H_out, W_out) --> (B, C, H_in, W_in)\n",
    "                  same as `self.gradprop(s*e)` or `(s*e).backward(); c=self.input.grad`\n",
    "        [Stpe 4]: (B, C, H_in, W_in) x (B, C, H_in, W_in) = (B, C, H_in, W_in)\n",
    "        \n",
    "        ```\n",
    "        # Step 1\n",
    "        s = self.output + eps \n",
    "        # Step 2\n",
    "        e = r / s\n",
    "        # Step 3:\n",
    "        c = self.gradprop(e)\n",
    "        # Step 4\n",
    "        r_next = self.input * c\n",
    "        ```\n",
    "        \"\"\"        \n",
    "        r = o[0]\n",
    "        eps = 1e-6\n",
    "        # Step 1\n",
    "        s = self.output + eps\n",
    "        # Step 2\n",
    "        e = r / s\n",
    "        # Step 3\n",
    "        c = self.gradprop(e)\n",
    "        # Step 4\n",
    "        r_next = self.input * c\n",
    "        \n",
    "        # for debugging\n",
    "        self.r = r  \n",
    "        self.r_next = r_next\n",
    "        \n",
    "        return (r_next,)\n",
    "    \n",
    "    def gradprop(self, x):\n",
    "        \"\"\"\n",
    "        get maxpooled switches first then unpool\n",
    "        \"\"\"\n",
    "        _, switches = torch.nn.functional.max_pool2d(self.input, \n",
    "                                                     self.module.kernel_size, \n",
    "                                                     self.module.stride, \n",
    "                                                     self.module.padding, \n",
    "                                                     self.module.dilation, \n",
    "                                                     self.module.ceil_mode, \n",
    "                                                     return_indices=True)\n",
    "        c = torch.nn.functional.max_unpool2d(x, switches, \n",
    "                                             self.module.kernel_size, \n",
    "                                             self.module.stride, \n",
    "                                             self.module.padding)\n",
    "        return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hook(m, i, o):\n",
    "    print(m)\n",
    "    print(type(i))\n",
    "    for tensor in i:\n",
    "        print(tensor)\n",
    "    print(type(o))\n",
    "    print(o)\n",
    "    print()\n",
    "relu = nn.ReLU()\n",
    "# relu.register_backward_hook(hook)\n",
    "maxpool_p = nn.MaxPool2d(2)\n",
    "\n",
    "a = relMaxPool2d(maxpool_p)\n",
    "# conv_p.register_backward_hook(hook)\n",
    "\n",
    "maxpool = nn.MaxPool2d(2)\n",
    "\n",
    "b = relMaxPool2d(maxpool)\n",
    "# conv.register_backward_hook(hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 2, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 1, 8, 8).requires_grad_(True)\n",
    "o1 = a(x)\n",
    "o2 = relu(o1)\n",
    "o3 = b(o2)\n",
    "o3.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if all equal returns True\n",
    "(x.grad == a.r_next).sum() == x.grad.view(-1).size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
