{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from collections import OrderedDict\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LRP Method\n",
    "\n",
    "$\\begin{aligned} r_i^{(L)} &= \\begin{cases}S_i(x) & \\text{if unit }i \\text{ is the target unit of interest}\\\\ 0 & \\text{otherwise}\\end{cases}  \\\\ r_i^{(l)} &= \\sum_j \\dfrac{z_{ji}}{\\sum_{i’}(z_{ji’}+b_j)+ \\epsilon \\cdot sign(\\sum_{i’}(z_{ji’}+b_j))}r_j^{(l+1)}\\\\ &\\text{where } z_{ji} = w_{ji}^{(l+1, l)}x_{i}^{(l)}\\end{aligned} $\n",
    "\n",
    "input feature $(1, \\cdots, i, \\cdots N)$, output feature $(1, \\cdots, j, \\cdots M)$\n",
    "\n",
    "weight $(N, M)$ is transposed to $(M, N)$ in pytorch\n",
    "\n",
    "$\\begin{aligned} X^{(l+1)} &= \\begin{bmatrix}  x_1 & \\cdots  &x_M \\end{bmatrix}^T\\\\\n",
    "X^{(l)} &= \\begin{bmatrix}  x_1 & \\cdots  & x_N \\end{bmatrix}^T\\\\\n",
    "W^{(l+1, l)} &= \\begin{bmatrix} \n",
    "w_{11} & \\cdots & w_{1i} & \\cdots & w_{1N} \\\\ \n",
    "\\vdots & \\ddots & \\ddots & \\ddots & \\vdots \\\\ \n",
    "w_{j1} & \\ddots & w_{ji} & \\ddots & w_{jN} \\\\\n",
    "\\vdots & \\ddots & \\ddots & \\ddots & \\vdots \\\\\n",
    "w_{M1} & \\cdots & w_{Mi} & \\cdots & w_{MN}\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}$\n",
    "\n",
    "to see element-wise calculation ...\n",
    "\n",
    "$\\begin{aligned} \n",
    "R^{(l)} &= \\begin{bmatrix} r_1 \\\\ \\vdots \\\\ r_i \\\\ \\vdots \\\\ r_N\\end{bmatrix}^{(l)} = \\begin{bmatrix} \\sum_j^M a_{1j}r_{1j} \\\\ \\vdots \\\\ \\sum_j^M a_{ij}r_{ij} \\\\ \\vdots \\\\ \\sum_j^M a_{Nj}r_{Nj} \\end{bmatrix}^{(l)} = \\begin{bmatrix} a_{11}r_{11} + \\cdots + a_{1M}r_{1M} \\\\ \\vdots \\\\ a_{i1}r_{i1} + \\cdots + a_{iM}r_{iM} \\\\ \\vdots \\\\ a_{N1}r_{N1} + \\cdots + a_{NM}r_{NM} \\end{bmatrix}^{(l)} \n",
    "\\\\\n",
    "Z^{(l, l+1)} &= \\begin{bmatrix} \n",
    "z_{11} & \\cdots & z_{1j} & \\cdots & z_{1M} \\\\ \n",
    "\\vdots & \\ddots & \\ddots & \\ddots & \\vdots \\\\ \n",
    "z_{i1} & \\ddots & z_{ij} & \\ddots & z_{ij} \\\\\n",
    "\\vdots & \\ddots & \\ddots & \\ddots & \\vdots \\\\\n",
    "z_{N1} & \\cdots & z_{Nj} & \\cdots & z_{NM}\n",
    "\\end{bmatrix} = \\begin{bmatrix} \n",
    "w_{11}x_1^{(l+1)} & \\cdots & w_{1j}x_j^{(l+1)} & \\cdots & w_{1M}x_M^{(l+1)} \\\\ \n",
    "\\vdots & \\ddots & \\ddots & \\ddots & \\vdots \\\\ \n",
    "w_{i1}x_1^{(l+1)} & \\ddots & w_{ij}x_j^{(l+1)} & \\ddots & w_{ij}x_M^{(l+1)} \\\\\n",
    "\\vdots & \\ddots & \\ddots & \\ddots & \\vdots \\\\\n",
    "w_{N1}x_1^{(l+1)} & \\cdots & w_{Nj}x_j^{(l+1)} & \\cdots & w_{NM}x_M^{(l+1)}\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}$\n",
    "\n",
    "### 1st way\n",
    "\n",
    "to get $r_i^{(l+1)}$ where $z_{ji}^{(l+1)} = w_{ji}^{(l+1, 1)} x_i^{(l)}$ there are 4 step in Linear Layer\n",
    "\n",
    "$\\begin{aligned} \n",
    "(1) & Z^{(l, l+1)} = W^{(l, l+1)} \\times X^{(l+1)}\\\\\n",
    "(2) & S^{(l+1)} = X^{(l+1)} + \\epsilon \\cdot sign(X^{(l+1)}) \\\\\n",
    "(3) & A^{(l, l+1)} = \\dfrac{Z^{(l, l+1)}}{S^{(l+1)}} \\\\\n",
    "(4) & R^{(l)} = A^{(l, l+1)}R^{(l+1)}  \\\\\n",
    "\\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd way\n",
    "\n",
    "same calculation but different order, introducing at http://heatmapping.org/tutorial/\n",
    "\n",
    "$\\begin{aligned} \n",
    "(1) & S^{(l+1)} = X^{(l+1)} + \\epsilon \\cdot sign(X^{(l+1)}) \\\\\n",
    "(2) & E^{(l+1)} = \\dfrac{R^{(l+1)}}{S^{(l+1)}} \\\\\n",
    "(3) & C^{(l)} = W^{(l, l+1)} E^{(l+1)} \\\\\n",
    "(4) & R^{(l)} = X^{(l)} \\times C^{(l)}  \\\\\n",
    "\\end{aligned}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relLinear(nn.Linear):\n",
    "    def __init__(self, linear):\n",
    "        super(nn.Linear, self).__init__()\n",
    "        self.in_features = linear.in_features\n",
    "        self.out_features = linear.out_features\n",
    "        self.weight = linear.weight  # (out_f, in_f)\n",
    "        self.bias = linear.bias  # out_f\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        self.register()\n",
    "        \n",
    "    def register(self):\n",
    "        self.register_forward_hook(self.hook_function)\n",
    "    \n",
    "    def hook_function(self, *x):\n",
    "        _, i, o = x\n",
    "        self.input = i[0].data\n",
    "        self.output = o.data\n",
    "        \n",
    "    def rho(self, w, use_rho):\n",
    "        if use_rho:\n",
    "            return torch.clamp(w, min=0)\n",
    "        else:\n",
    "            return w\n",
    "        \n",
    "    def relprop(self, r, use_rho=False):\n",
    "        \"\"\"\n",
    "        lrp method\n",
    "            > * must run after `self.forward`\n",
    "            > \n",
    "            > forward shape\n",
    "            > input: (B, *, in_f)\n",
    "            > output: (B, *, out_f)\n",
    "\n",
    "        - relprop shape\n",
    "        r = (l+1)-th layer: (B, *, out_f)\n",
    "        r_next = l-th layer: (B, *, in_f)\n",
    "        \n",
    "        if rho==True:\n",
    "        function rho(w) is applied\n",
    "        \"\"\"\n",
    "        eps = 1e-6\n",
    "        w = self.rho(self.weight, use_rho)\n",
    "        ### implementation method 1\n",
    "        ## (B, in_f, 1) * (1, in_f, out_f) = (B, in_f, out_f)\n",
    "        #z = self.input.unsqueeze(-1) * self.rho(self.weight).transpose(0, 1).unsqueeze(0)\n",
    "        ## (B, 1, out_f)\n",
    "        #s = self.output.unsqueeze(1) + eps * torch.sign(self.output.unsqueeze(1))  \n",
    "        #weight = z / s\n",
    "        ## (B, in_f, out_f) x (B, out_f, 1) = (B, in_f)\n",
    "        #r_next = torch.bmm(weight, r.unsqueeze(-1)).squeeze()\n",
    "        \n",
    "        ### implemetation method 2\n",
    "        # Step 1: (B, out_f) \n",
    "        s = self.output + eps * torch.sign(self.output)  \n",
    "        # Step 2: (B, out_f) / (B, out_f) = (B, out_f)\n",
    "        e = r / s\n",
    "        # Step 3: (B, in_f, out_f) * (B, out_f, 1) = (B, in_f)\n",
    "        c = torch.bmm(w.transpose(0, 1).expand(e.size(0), self.in_features, self.out_features), \n",
    "                      e.unsqueeze(-1)).squeeze(-1)\n",
    "        # Step 4: (B, in_f) x (B, in_f) = (B, in_f)\n",
    "        r_next = self.input * c\n",
    "        \n",
    "        assert r_next.size(1) == self.in_features, \"size of `r_next` is not correct\"\n",
    "        return r_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nn.Linear(3, 2)\n",
    "b = relLinear(a)\n",
    "o = b(torch.rand(5, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_vector = torch.zeros(5, 2).scatter(1, torch.LongTensor([[1], [0], [0], [1], [0]]), 1)\n",
    "target_vector  # l+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.2037e+00, -2.5058e+01,  5.0031e+00],\n",
       "        [-1.0748e-01, -6.5016e-01, -1.5516e+00],\n",
       "        [-4.7735e-03, -1.1511e-01, -6.3940e-01],\n",
       "        [ 1.3798e-01, -1.8393e-01,  9.8508e-02],\n",
       "        [-5.0873e-02, -2.8419e-01, -1.1969e+00]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.relprop(target_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In convolutional layer to get $r_i^{(l+1)}$ there are 4 step in Conv Layer, but change step 3 computing gradient of conv. which can be replaced as Transposed convolutional layer(=fractionally strided convolutional layer)\n",
    "\n",
    "$\\begin{aligned} \n",
    "(1) & S^{(l+1)} = X^{(l+1)} + \\epsilon \\cdot sign(X^{(l+1)}) \\\\\n",
    "(2) & E^{(l+1)} = \\dfrac{R^{(l+1)}}{S^{(l+1)}} \\\\\n",
    "(3) & C^{(l)} = \\triangledown (\\sum S^{(l+1)} \\times E^{(l+1)}) \\\\\n",
    "(4) & R^{(l)} = X^{(l)} \\times C^{(l)}  \\\\\n",
    "\\end{aligned}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relReLU(nn.ReLU):\n",
    "    \n",
    "    def relprop(self, r): \n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relConv2d(nn.Conv2d):\n",
    "    def __init__(self, conv2d):\n",
    "        super(nn.Conv2d, self).__init__(conv2d.in_channels, \n",
    "                                        conv2d.out_channels, \n",
    "                                        conv2d.kernel_size, \n",
    "                                        conv2d.stride, \n",
    "                                        conv2d.padding,\n",
    "                                        conv2d.dilation,\n",
    "                                        conv2d.transposed,\n",
    "                                        conv2d.output_padding,\n",
    "                                        conv2d.groups, \n",
    "                                        None,  # init of bias\n",
    "                                        conv2d.padding_mode)\n",
    "        self.weight = conv2d.weight\n",
    "        self.bias = conv2d.bias\n",
    "        \n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        self.register()\n",
    "        \n",
    "    def register(self):\n",
    "        self.register_forward_hook(self.hook_function)\n",
    "    \n",
    "    def hook_function(self, *x):\n",
    "        _, i, o = x\n",
    "        self.input = i[0].data\n",
    "        self.output = o.data\n",
    "    \n",
    "    def rho(self, w, use_rho):\n",
    "        if use_rho:\n",
    "            return torch.clamp(w, min=0)\n",
    "        else:\n",
    "            return w\n",
    "    \n",
    "    def cal_output_padding(self):\n",
    "        \"\"\"\n",
    "        calculate output_padding size\n",
    "        - size of height or width: (X_in + 2P - K) / S + 1 = X_out\n",
    "        - output_padding = X_in - ((X_out - 1) * S + K - 2P)\n",
    "\n",
    "        * what is output_padding?\n",
    "        from PyTorch Document:\n",
    "        https://pytorch.org/docs/stable/nn.html#convtranspose2d\n",
    "\n",
    "        The padding argument effectively adds `dilation * (kernel_size - 1) - padding` amount of zero padding to \n",
    "        both sizes of the input. This is set so that when a `Conv2d` and a `ConvTranspose2d` are initialized with \n",
    "        same parameters, they are inverses of each other in regard to the input and output shapes. \n",
    "        However, when `stride > 1`, `Conv2d` maps multiple input shapes to the same output shape. \n",
    "        `output_padding` is provided to resolve this ambiguity by effectively increasing \n",
    "        the calculated output shape on one side. Note that output_padding is only used to find output shape, \n",
    "        but does not actually add zero-padding to output.\n",
    "        \"\"\"\n",
    "        H_in, W_in = self.input.size()[2:]\n",
    "        H_out, W_out = self.output.size()[2:]\n",
    "        S_in, S_out = self.stride\n",
    "        K_in, K_out = self.kernel_size\n",
    "        P_in, P_out = self.padding\n",
    "        H_output_padding = H_in - ((H_out - 1)*S_in + K_in - 2*P_in)\n",
    "        W_output_padding = W_in - ((W_out - 1)*S_out + K_out - 2*P_out)\n",
    "        return (H_output_padding, W_output_padding)\n",
    "    \n",
    "    def gradprop(self, x):\n",
    "        \"\"\"\n",
    "        `ConvTransposed2d` can be seen as the gradient of `Conv2d` with respect to its input.\n",
    "        \"\"\"\n",
    "        output_padding = self.cal_output_padding()\n",
    "        c = torch.nn.functional.conv_transpose2d(x, weight=self.weight, stride=self.stride, \n",
    "                                                 padding=self.padding, output_padding=output_padding)\n",
    "        return c\n",
    "    \n",
    "    def relprop(self, r, use_rho=False):\n",
    "        \"\"\"\n",
    "        lrp method\n",
    "            > * must run after `self.forward`\n",
    "            > \n",
    "            > forward shape\n",
    "            > input: (B, C_in, H, W)\n",
    "            > output: (B, C_out, H_out, W_out)\n",
    "\n",
    "        - relprop shape\n",
    "        r = (l+1)-th layer: (B, C_out, H_out, W_out)\n",
    "        r_next = l-th layer: (B, C_in, H, W)\n",
    "        \n",
    "        if rho==True:\n",
    "        function rho(w) is applied\n",
    "        \"\"\"\n",
    "        eps = 1e-6\n",
    "        w = self.rho(self.weight, use_rho)\n",
    "        # Step 1: (B, C_out, H_out, W_out) \n",
    "        s = self.output + eps * torch.sign(self.output)  \n",
    "        # Step 2: (B, C_out, H_out, W_out) / (B, C_out, H_out, W_out) = (B, C_out, H_out, W_out)\n",
    "        e = r / s\n",
    "        # Step 3: (B, C_out, H_out, W_out) --> (B, C_in, H, W)\n",
    "        # same as `self.gradprop(s*e)` or `(s*e).backward(); c=self.input.grad`\n",
    "        c = self.gradprop(e)\n",
    "        # Step 4: (B, C_in, H, W) x (B, C_in, H, W) = (B, C_in, H, W)\n",
    "        r_next = self.input * c\n",
    "        return r_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nn.Conv2d(1, 32, 3)\n",
    "b = relConv2d(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = b(torch.randn(2, 1, 28, 28))\n",
    "r = torch.relu(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 28, 28])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_next = b.relprop(r)\n",
    "r_next.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maxpooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relMaxPool2d(nn.MaxPool2d):\n",
    "    def __init__(self, maxpool2d):\n",
    "        super(nn.MaxPool2d, self).__init__(maxpool2d.kernel_size,\n",
    "                                           maxpool2d.stride,\n",
    "                                           maxpool2d.padding,\n",
    "                                           maxpool2d.dilation,\n",
    "                                           maxpool2d.return_indices,\n",
    "                                           maxpool2d.ceil_mode)    \n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        self.register()\n",
    "\n",
    "    def register(self):\n",
    "        self.register_forward_hook(self.hook_function)\n",
    "    \n",
    "    def hook_function(self, *x):\n",
    "        _, i, o = x\n",
    "        self.input = i[0].data\n",
    "        self.output = o.data\n",
    "        \n",
    "    def gradprop(self, x):\n",
    "        _, switches = torch.nn.functional.max_pool2d(self.input, self.kernel_size, self.stride, self.padding, \n",
    "                                                     self.dilation, self.ceil_mode, return_indices=True)\n",
    "        c = torch.nn.functional.max_unpool2d(x, switches, self.kernel_size, self.stride, self.padding)\n",
    "        return c\n",
    "    \n",
    "    def relprop(self, r):\n",
    "        \"\"\"\n",
    "        lrp method\n",
    "            > * must run after `self.forward`\n",
    "            > \n",
    "            > forward shape\n",
    "            > input: (B, C, H, W)\n",
    "            > output: (B, C, H_out, W_out)\n",
    "\n",
    "        - relprop shape\n",
    "        r = (l+1)-th layer: (B, C, H_out, W_out)\n",
    "        r_next = l-th layer: (B, C, H, W)\n",
    "        \"\"\"\n",
    "        eps = 1e-6\n",
    "        # Step 1: (B, C, H_out, W_out) \n",
    "        s = self.output + eps * torch.sign(self.output)  \n",
    "        # Step 2: (B, C, H_out, W_out) / (B, C, H_out, W_out) = (B, C, H_out, W_out)\n",
    "        e = r / s\n",
    "        # Step 3: (B, C, H_out, W_out) --> (B, C, H, W)\n",
    "        # same as `self.gradprop(s*e)` or `(s*e).backward(); c=self.input.grad`\n",
    "        c = self.gradprop(e)\n",
    "        # Step 4: (B, C, H, W) x (B, C, H, W) = (B, C_in, H, W)\n",
    "        r_next = self.input * c\n",
    "        return r_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nn.MaxPool2d(2)\n",
    "b = relMaxPool2d(a)\n",
    "x = torch.randn(2, 32, 26, 26)\n",
    "output = b(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 32, 26, 26])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_next = b.relprop(output)\n",
    "r_next.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        reshape layer\n",
    "        - forward: flatten at convs > linear\n",
    "        - backward: unflatten at linear > convs     \n",
    "        \"\"\"\n",
    "        super(Reshape, self).__init__()\n",
    "    \n",
    "    def __name__(self):\n",
    "        return \"Reshape\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        reshape input to output\n",
    "        input: (B, C, H, W)\n",
    "        output: (B, C*H*W)\n",
    "        \"\"\"\n",
    "        self.B, self.C, self.H, self.W = x.size()\n",
    "        return x.view(self.B, -1)\n",
    "    \n",
    "    def relprop(self, x):\n",
    "        \"\"\"\n",
    "        lrp method\n",
    "            > * must run after `self.forward`\n",
    "            > \n",
    "            > forward shape\n",
    "            > input: (B, C, H, W)\n",
    "            > output: (B, C*H*W)\n",
    "\n",
    "        - relprop shape\n",
    "        r = (l+1)-th layer: (B, C*H*W)\n",
    "        r_next = l-th layer: (B, C, H, W)\n",
    "        \"\"\"\n",
    "        return x.view(-1, self.C, self.H, self.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = Reshape()\n",
    "x = torch.rand(2, 1, 12, 12)\n",
    "output = rs(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lrpMNIST(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(lrpMNIST, self).__init__()\n",
    "        \n",
    "        # lrp\n",
    "        self.activation_func = model.activation_func\n",
    "        self.model_type = model.model_type\n",
    "        self.activation_type = model.activation_type\n",
    "        self.convs_len = model.convs_len\n",
    "        self.fc_len = model.fc_len\n",
    "        self.convs = model.convs\n",
    "        self.fc = model.fc\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"cnn\"\n",
    "activation_type = \"relu\"\n",
    "m = deepcopy(MNISTmodel(model_type, activation_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (1): ReLU()\n",
       "  (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (3): ReLU()\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.convs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTmodel(nn.Module):\n",
    "    \"\"\"\n",
    "    paper implementation 'https://arxiv.org/abs/1711.06104'\n",
    "    \n",
    "    model_type: \n",
    "        - DNN: \n",
    "            Reshape: 28,28 > 28*28\n",
    "            Linear: 28*28, 512 \n",
    "            Linear: 512, 512\n",
    "            Linear: 512, 10\n",
    "        - CNN: \n",
    "            Conv2d: (1, 32, 3)\n",
    "            Conv2d: (32, 64, 3)\n",
    "            MaxPool2d: (2)\n",
    "            Reshape: 12,12 > 12*12\n",
    "            Linear: (64*12*12, 128)\n",
    "            Linear: (128, 10)\n",
    "\n",
    "            ** Conv2d = (in_kernels, out_kernels, kernel_size)\n",
    "            ** MAxPool2d = (kernel_size)\n",
    "\n",
    "    activation_type:\n",
    "        - ReLU, Tanh, Sigmoid, Softplus\n",
    "    \"\"\"\n",
    "    def __init__(self, model_type, activation_type):\n",
    "        \"\"\"\n",
    "        model_type: \"dnn\", \"cnn\"\n",
    "        activation_type: \"relu\", \"tanh\", \"sigmoid\", \"softplus\"\n",
    "        \"\"\"\n",
    "        super(MNISTmodel, self).__init__()\n",
    "        self.model_type = model_type\n",
    "        self.activation_type = activation_type\n",
    "        self.act = {\"relu\": nn.ReLU, \n",
    "                    \"tanh\": nn.Tanh, \n",
    "                    \"sigmoid\": nn.Sigmoid, \n",
    "                    \"softplus\": nn.Softplus}\n",
    "        \n",
    "        self.layers = self.make_layers(model_type, activation_type)\n",
    "        \n",
    "        if model_type == \"cnn\":\n",
    "            self.switches = OrderedDict()\n",
    "            self.maxpool2d_locs = []\n",
    "            \n",
    "        self.activation_maps = OrderedDict()\n",
    "        self.activation_locs = []\n",
    "        \n",
    "    def make_layers(self, model_type, activation_type):\n",
    "        self.activation_func = self.act[activation_type.lower()]\n",
    "        if model_type.lower() == \"dnn\":\n",
    "            layers = nn.Sequential(\n",
    "                Reshape(),\n",
    "                nn.Linear(28*28, 512),\n",
    "                self.activation_func(), \n",
    "                nn.Linear(512, 512),\n",
    "                self.activation_func(), \n",
    "                nn.Linear(512, 10),\n",
    "            )\n",
    "        elif model_type.lower() == \"cnn\":\n",
    "            layers = nn.Sequential(\n",
    "                nn.Conv2d(1, 32, 3),\n",
    "                self.activation_func(),\n",
    "                nn.Conv2d(32, 64, 3),\n",
    "                self.activation_func(), \n",
    "                nn.MaxPool2d(2, return_indices=True),\n",
    "                Reshape(),\n",
    "                nn.Linear(64*12*12, 128),\n",
    "                self.activation_func(),\n",
    "                nn.Linear(128, 10)\n",
    "            )\n",
    "        else:\n",
    "            assert False, \"please insert `model_type` = `dnn` or `cnn`\"\n",
    "        return layers\n",
    "    \n",
    "    def save_activation_maps(self, layer, idx, x, typ):\n",
    "        if isinstance(layer, self.activation_func):\n",
    "            layer_name = f\"{typ}{idx}\"\n",
    "            self.activation_locs.append(layer_name)  # only for record\n",
    "            self.activation_maps[layer_name] = x\n",
    "    \n",
    "    def forward(self, x, store=False):\n",
    "        \"\"\"\n",
    "        store: if True, save activation maps\n",
    "        \"\"\"\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            if isinstance(layer, nn.MaxPool2d):\n",
    "                x, indices = layer(x)\n",
    "                self.switches[idx] = indices  # for deconvnet\n",
    "                self.maxpool2d_locs.append(idx)  # only for record\n",
    "            else:\n",
    "                x = layer(x)\n",
    "                if store:\n",
    "                    self.save_activation_maps(layer, idx, x, typ=layer.__name__)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class deconvMNIST(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(deconvMNIST, self).__init__()\n",
    "                \n",
    "        # deconv\n",
    "        self.activation_func = model.activation_func\n",
    "        self.model_type = model.model_type\n",
    "        self.activation_type = model.activation_type\n",
    "\n",
    "        if self.model_type == \"cnn\":\n",
    "            self.switches = model.switches\n",
    "        self.convs, self.fc = self.deconv_make_layers()\n",
    "        \n",
    "    def deconv_make_layers(self):\n",
    "        if self.model_type.lower() == \"dnn\":\n",
    "            convs = None\n",
    "            fc = nn.Sequential(\n",
    "                nn.Linear(10, 512),\n",
    "                self.activation_func(), \n",
    "                nn.Linear(512, 512),\n",
    "                self.activation_func(), \n",
    "                nn.Linear(512, 28*28),\n",
    "            )\n",
    "        elif self.model_type.lower() == \"cnn\":\n",
    "            fc = nn.Sequential(\n",
    "                nn.Linear(10, 128),\n",
    "                self.activation_func(),\n",
    "                nn.Linear(128, 64*12*12)    \n",
    "            )\n",
    "            convs = nn.Sequential(\n",
    "                nn.MaxUnpool2d(2),\n",
    "                self.activation_func(), \n",
    "                nn.ConvTranspose2d(64, 32, 3),\n",
    "                self.activation_func(),\n",
    "                nn.ConvTranspose2d(32, 1, 3),\n",
    "            )\n",
    "        else:\n",
    "            assert False, \"[init error] `model` doesn't have `model_type` or `activation_func` attrs\"\n",
    "        return convs, fc\n",
    "    \n",
    "    def forward(self, t):\n",
    "        x = self.fc(t)\n",
    "        if self.convs is not None:\n",
    "            x = x.view(x.size(0), 64, 12, 12)\n",
    "            for idx, layer in enumerate(self.convs):\n",
    "                if isinstance(layer, nn.MaxUnpool2d):\n",
    "                    x = layer(x, self.switches[self.convs_len - 1 - idx])\n",
    "                else:\n",
    "                    x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class saliencyMNIST(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(saliencyMNIST, self).__init__()\n",
    "        \"\"\"\n",
    "        model_type: \"dnn\", \"cnn\"\n",
    "        activation_type: \"relu\", \"tanh\", \"sigmoid\", \"softplus\"\n",
    "        \"\"\"\n",
    "        super(saliencyMNIST, self).__init__()\n",
    "\n",
    "        self.activation_func = model.activation_func\n",
    "        self.model_type = model.model_type\n",
    "        self.activation_type = model.activation_type\n",
    "        self.convs_len = model.convs_len\n",
    "        self.fc_len = model.fc_len\n",
    "        # TODO: rewrite code\n",
    "        self.model = deepcopy(model)\n",
    "        self.model.eval()\n",
    "        \n",
    "    def generate_saliency(self, x, target):\n",
    "        \"\"\"vanilla gradient*input\"\"\"\n",
    "        x.requires_grad_(requires_grad=True)\n",
    "        self.model.zero_grad()\n",
    "        output = self.model(x)\n",
    "        grad_outputs = torch.zeros_like(output)\n",
    "        grad_outputs[:, target] = 1\n",
    "\n",
    "        output.backward(gradient=grad_outputs)\n",
    "        x.requires_grad_(requires_grad=False)\n",
    "        \n",
    "        return x.grad.clone() * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(root, batch_size, download=False):\n",
    "    train_dataset = datasets.MNIST(\n",
    "        root=root,                                \n",
    "        train=True,\n",
    "        transform=transforms.Compose([\n",
    "           transforms.ToTensor(),\n",
    "           transforms.Normalize((0.5,), (0.5,))  # normalize to (-1, 1)\n",
    "        ]),\n",
    "        download=download)\n",
    "    test_dataset = datasets.MNIST(\n",
    "        root=root, \n",
    "        train=False,\n",
    "        transform=transforms.Compose([\n",
    "           transforms.ToTensor(),\n",
    "           transforms.Normalize((0.5,), (0.5,))\n",
    "        ]),\n",
    "        download=download)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size, \n",
    "        shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size, \n",
    "        shuffle=True)\n",
    "    return train_dataset, test_dataset, train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_types = [\"relu\", \"tanh\", \"sigmoid\", \"softplus\"]\n",
    "model_types = [\"dnn\", \"cnn\"]\n",
    "args = dict(\n",
    "    root = Path().home()/\"code\"/\"data\",\n",
    "    project_path = Path().home()/\"code\"/\"XAI\",\n",
    "    logterm = False, \n",
    "    sv_folder = \"mnist\", \n",
    "    n_step = 20,\n",
    "    batch_size = 128,\n",
    "    download = False,\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    seed = 73\n",
    ")\n",
    "train_dataset, test_dataset, train_loader, test_loader = build_dataset(str(args[\"root\"]), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "torch.manual_seed(74)\n",
    "\n",
    "test_dataset.\n",
    "for i in range(10):\n",
    "    for imgs, labels in test_loader:\n",
    "        \n",
    "        break\n",
    "    \n",
    "num = 1\n",
    "sample = imgs[:num]\n",
    "sample_label = labels[:num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"cnn\"\n",
    "activation_type = \"relu\"\n",
    "model = MNISTmodel(model_type, activation_type)\n",
    "model.load_state_dict(torch.load(f\"./trained/mnist/{model_type}-{activation_type}.pt\", map_location=\"cpu\"))\n",
    "\n",
    "output = model(sample, store=True)\n",
    "pred = output.argmax(1)\n",
    "target_vector = torch.zeros_like(output).scatter(1, sample_label.unsqueeze(0), 1).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff545890128>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "saliency = saliencyMNIST(model)\n",
    "vanilla_saliency = saliency.generate_saliency(sample, pred)\n",
    "plt.imshow(vanilla_saliency.squeeze()*0.5+0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deconv = deconvMNIST(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 12, 12])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.switches[4].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_type, activation_type, load=True):\n",
    "    model = MNISTmodel(model_type, activation_type)\n",
    "    ex_model = explainMNISTmodel(model_type, activation_type)\n",
    "    if load:\n",
    "        model.load_state_dict(torch.load(f\"./trained/mnist/{model_type}-{activation_type}.pt\", map_location=\"cpu\"))\n",
    "    return model, ex_model\n",
    "\n",
    "def run_backward(x, oc_x, target, model):\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    ret = []\n",
    "    for img in [x, oc_x]:\n",
    "        img = img.requires_grad_()\n",
    "        model.zero_grad()\n",
    "        output = model(img, store=True)\n",
    "        pred = output.argmax(1) \n",
    "        loss = loss_function(output, target)\n",
    "        loss.backward()\n",
    "        ret.append((img.grad, pred))\n",
    "    return ret\n",
    "\n",
    "def unnormalize(x, mean=0.5, std=0.5):\n",
    "    return (x*0.5) + 0.5\n",
    "\n",
    "def draw_activaion_map(model):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(6, 24))\n",
    "\n",
    "    for ax, layer_name in zip(axes, model.activation_locs):\n",
    "        if \"conv\" in layer_name:\n",
    "            act_map = model.activation_maps[layer_name].max(1)[0].detach().squeeze()\n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "            im = ax.imshow(act_map, cmap=\"gray\")\n",
    "            fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "            ax.set_title(f\"{layer_name} \\n{list(act_map.size())}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def draw_gradient_map(grad, oc_grad, inp, oc_inp):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(6, 6))\n",
    "    titles = [\"original\\ngradient\", \"occluded\\ngradient\", \"gradient*input\", \"occluded\\ngradient*input\"]\n",
    "    gradients = [grad, oc_grad, inp.detach()*grad, oc_inp.detach()*grad]\n",
    "    \n",
    "    for ax, g, t in zip(axes.flatten(), gradients, titles):\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "        im = ax.matshow(g.squeeze(), cmap=\"coolwarm\")\n",
    "        ax.xaxis.tick_bottom()\n",
    "        fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "        ax.set_title(f\"{t}\")\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAADHCAYAAAAJSqg8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAFyBJREFUeJzt3Xu0XGV5x/HvLyESMCC3AiGJSYQsClIL5doCFopcpE2BIi5Z2FKojVaCpKIlpNqgbZEqAsXrikKBhoIUI1IRKEQstmBqEMrFQHOBkkAgRJCEpgsJPP1jv0d3zuxzzsyZ25n3/D5rnXVm3v3uvd8988wz77z7pojAzMx635huN8DMzFrDCd3MLBNO6GZmmXBCNzPLhBO6mVkmnNDNzDLhhD4KSPpVSZu73Q7rPkkXSVo4zHmPkrSmHfNKekXS24azbPulLBJ6Coa+vzck/V/p+Rkdbst4SSFp8iB1PpTqnNuvfL2kw9rfyvaSdKSk70l6UdILkm6QtGu329VrJP2xpEckbZL0nKSvSNqh2+1qh4iYEBGrut2ORkg6U9IDkjZIWiPps5K26mabskjoKRgmRMQE4GlgZqns+kaW1cE35EXgLyVt26H1tUSdr88OwBeBqcA04A1gQRublR1J5wN/B3wceAtwGMXreZekN3WzbfYL2wJzgF2AQ4FjgI91s0FZJPShSDpc0hJJL0t6VtLlfYmp1KP+M0krgUdT+e9KWi7pZ5KukPRDSe8vLfODkp5IvdDbJE1Kk+5N/59IvxBOHqBZDwKPAOdWTZR0o6RPlJ6fIGlF6flzkj4q6bG0nq9ImijprtRjuEPS9v2W+SFJa9NrcG6pfKykT0palX4lXN/XE+wbrpH0p5JWA98d6vWOiH+JiEURsTEi/hf4EnD4UPNZIb1vnwLOjYg7IuK1iHgKeC9FUn9/qjdW0jxJKyVtTL3FKWna21MsvCjpeUnzKtZTMwwi6SlJ70qPt5F0jaSXJP0EOLhf3T0kfTP9CntS0kdK0wadt6ItIWmv9PgaSV+WdHuK7f+QtHv6HL4k6XFJB5TmnVt6DX4i6ZTStLGSPp/i+klJs9O6+j7/b5F0VfpcPCPpbySNHfpdgoj4SkT8ICJ+HhHPANfT5TgfFQkdeA2YDewEHAnMBD7Qr87vAQcCB0jaHfgG8OfArwDPpmkASHofxTfzTGA3iuTcNy75zvR/7/QL4ZZB2vUJ4C/6J94GnAL8NrAv8D7g28BHU5smAH9WqjsW+E3gbcDvAp+SdESa9nHgOOAIYDLF63V5v3kPBfYGTgJIX2Z/UGc73wk81uC2jWa/BYwHFpULI+IV4Hbg2FT0UeB04ERge+BsYJOk7YC7gTuAPYC9gMXDaMd8YM/0dzxwZt8ESWOAfwH+C5hE0TudI+n4oeat03spPh+7AK8C9wM/Ts9vBi4r1V1J8bl+C8UX4UJJE9O0PwXeDewP/AbQv4N1LbCZ4jU6gOJz8IG0jW9NHbq31tnm7sd5RGT1BzwFvGuIOnOBG9Lj8UAAv1WaPgu4p/R8DLAOeH96fg9wRmn6OIokuFtpeZMHWf+HgLvT41uBT6XH64HD0uMbgU+U5jkBWFF6/hxwaun5bcDlpecfB25Mj381tWlaafqVwJfS4yeBw0vTpgObAJXm3WOY78eBwEvAod2OjV75o+iBPzfAtEuAu9LjJ4CTKuqcDjw4wPwXAQvT46OANf2m/+LzA6wCTihNm9VXn+IL/ul+814I/MNQ8w7QrgD2So+vAb5WmnYusKz0/NeAnw2yrIf6Xhfge8AHS9Pelda1Vfq8vgps0++1u2cY79lZwBpgl27GTlcH8DtF0r7A5ym+obeheDP/o1+11aXHe5SfR8Qbkp4pTZ8KfFXSl0plmyl6ty832LxPAv8m6coG5wN4vvT4/yqeT+hXv7yN/wMcIUnAFOC7kspXahsD7JwevxERzzbaOEn7AN+h+EAtaXT+UWw9sIukrSKi/9FJE9N0KN63lRXzD1TeqC0+BxQx02cqsIekn5XKxgI/qGPeetQd25L+iOLXyrRUNIGiJ1/VjvLjqRSdsbXFxwAo4r5cZ0hpWPUSii/C9UPVb6fRMuTyNYqfa3tGxPbApyl6n2XlZLaWIjkDv/h5Oak0fTXwxxGxQ+lvm4h4oN9yhhQR/0Xx0/iCfpP+l2KnS5/dG1nuAKaUHr8VeDaK7sUzwO/0257xpeBs+JKckvYE7gLmRcRNTbd8dLmfoue4xZCWpDdTDB/0DZ+sphjS6G+g8v62iLE0dvwrpelrqY2Z8jqe7Bcz20XEiXXM2zKSplJ8vmcDO0fEDhT7wfo+31t8lvu1aTXF67xLaRu2j4i3N7D+E9L6Z0bEI01sSkuMloS+HfByRLwi6e0U42qDuRU4VNKJaefJR4EdS9O/CnxC0t4AknaUdCpARLxK0Utv5Jja+RTDMOUE/hDwe5J2ULHDtXLnaYPmp51Vvw78IcV+Aii255LSDrVdJc0c7krSh+x7wN9FxD802+jRJiJephgL/oKKneHjJE0D/pniZ/0/pqpfB/5a0gwV3iFpZ4pfRbtLmiNpa0nbSTq0YlX/DYxXcQDAOIox661L028CLkzxPZktY/A/gQ2SLkgxNVbSfpIOrmPeVnozRYfjBQBJZwH79duG8yRNUrGj/xcdp4hYC/wr8HlJ20saI2lPSb9dz4ol/Q7FjtBTI+I/W7M5zRktCf3PgQ9IeoXiiItvDFY5vdGnU4wzr6f4hn+E4tuciLiB4rC8RZI2UCTfY0uL+Cvgn9MOld8fqnER8QTFjp5tSsVXAysoDsP8DnDD0Js5qNeBJRTj5XcAn46IviNyPkuxE+17kjYC91EMTw0oHVVw6gCTP0TRI/uMfnk+QFd/ivaaiPgsMA+4FNhA8d6tBo5JnQYodgzeRJGUNgBXUYwHb6SIx5kU+1qWA0dXrONl4MMUXwzPUPTYy0e9fIpiqOTJtI5/LM37elr+/mn6+rSctww1bytFxE8ohlPvpxiW+TW2HE79Wlr/wxQHL3yXYnj09TT9j4A3AT+h2NdzM8WwVt9O0VcG2Sn6SYrt/W4pzm9v4eY1TGlA3waReunPUfysur/b7TGz4ZH0buCrETG1221ph9HSQ2+YpHenY1THUwyJbAIe6HKzzKwBaTjoRElbpaHL+cC3ut2udnFCH9g7KX4urqM4xvaUiPh5d5tkZg0SxfDPSxRDLssohkSz5CEXM7NMuIduZpaJphJ6OqTqCUkrJM1tVaPMus2xbb1o2EMu6SSE/6Y4PGoN8CPg9HQY0UDzeHzH2ioi+p8w1jDHto1E9cR2Mz30QyiuLbIq7Sy8kXThJrMe59i2ntRMQp/Eltc8WMOWp8cDIGmWpKWSljaxLrNOcmxbT2rm4lxV3f+an50RsYB0cwP/LLUe4di2ntRMD30NW17oZjLFdcPNep1j23pSMwn9R8AMSdNV3BLrfRQXtTLrdY5t60nDHnKJiM2SZgN3UlwH+eqI8F1prOc5tq1XdfRMUY8zWru14rDF4XBsW7u1+7BFMzMbQZzQzcwy4YRuZpYJJ3Qzs0w4oZuZZcIJ3cwsE07oZmaZcEI3M8uEE7qZWSac0M3MMuGEbmaWCSd0M7NMOKGbmWWimTsWWQtNnTq1pmz27NmVdadNm1ZTduqpp9a9Lqn6om0LFy6sKZszZ05l3Z/+9Kd1r89GN8d257iHbmaWCSd0M7NMOKGbmWXCCd3MLBNN3YJO0lPARuB1YHNEHDRE/VF1m6799tuvsvzSSy+tKTv00ENryrbffvuWt6lR9957b2X50Ucf3eGW1KdVt6BzbA/Osd159cR2K45yOToi1rdgOWYjjWPbeoqHXMzMMtFsQg/gXyU9IGlWKxpkNkI4tq3nNDvkcnhEPCtpV+AuSY9HxBYDU+nD4A+E9RrHtvWcpnroEfFs+r8O+BZwSEWdBRFx0FA7lcxGEse29aJh99AlvRkYExEb0+PjgE+3rGU9ZsyY2u/GW265pbLu9OnT292clpk8eXK3m9Bxju0tObZ7RzNDLrsB30rXTtgK+KeIuKMlrTLrLse29aRhJ/SIWAX8egvbYjYiOLatV/mwRTOzTDihm5llwtdDb5GqnUEDXVd506ZNdS3zhz/8YWX50qVL627XjjvuWFN28cUX1z3/l7/85brrWp4c273DPXQzs0w4oZuZZcIJ3cwsE07oZmaZcEI3M8uEj3JpkZUrV9aUVV3Yv9OqTm9u5EgAM8d273AP3cwsE07oZmaZcEI3M8uEE7qZWSa8UzRzBx54YFPzP/DAAy1qidnwRERl+cknn9zUcnOMbffQzcwy4YRuZpYJJ3Qzs0w4oZuZZWLIhC7paknrJD1aKttJ0l2Slqf/tRcmNhvhHNuWm3qOcrkG+CJwXalsLrA4Ii6RNDc9v6D1zbNmHXbYYU3Nv2LFiha1ZES6Bsf2iJdu1l3jM5/5TFPLzTG2h+yhR8S9wIv9ik8Crk2PrwWaO37IrAsc25ab4Y6h7xYRawHS/11b1ySzrnJsW89q+4lFkmYBs9q9HrNOc2zbSDPcHvrzkiYCpP/rBqoYEQsi4qCIOGiY6zLrJMe29azh9tBvBc4ELkn/v92yFllLzZgxo6n5t9pq1F0dwrHdIxzbteo5bPEG4H5gb0lrJP0JRbAfK2k5cGx6btZTHNuWmyG/oiLi9AEmHdPitph1lGPbcuMzRc3MMuGEbmaWCSd0M7NM5Lebd5TaeuutK8uPP/74updx3XXX1ZStXr162G0yawXHdv3cQzczy4QTuplZJpzQzcwy4YRuZpYJ7xQdxB577FFTNmXKlKaXu3Llypqy9evX1z3/XnvtVVN25plnVtbddttta8qWL19eWfemm26qKRvojuvW2xzbeca2e+hmZplwQjczy4QTuplZJpzQzcwyoU7uGJDUsZW95z3vqSz/2Mc+VvcyJk6cWFM2efLkYbepz6pVq2rK7rvvvpqyu+++u3L+D3/4wzVlhxxySGXdF154oaZsn332qaz70ksvVZb3koiovqNwmzm2C47t9qkntt1DNzPLhBO6mVkmnNDNzDLhhG5mlol67il6taR1kh4tlV0k6RlJD6W/E9vbTLPWc2xbboY8ykXSO4FXgOsiYr9UdhHwSkRc2tDKmjwSYKC7dM+fP7+m7MILLxyoDTVlr776amXdjRs31pRVnW48UPmmTZvqrtsuixYtqik77bTTOrb+TmvkKBfH9pYc2yNbS45yiYh7gRdb0iKzEcSxbblpZgx9tqSH08/WHVvWIrPuc2xbTxpuQv8KsCewP7AW+PxAFSXNkrRU0tJhrsuskxzb1rOGldAj4vmIeD0i3gC+BlSfylXUXRARB0XEQcNtpFmnOLatlw3reuiSJkbE2vT0FODRweq3yhFHHFFZPm/evLqX8fWvf72mrOoGsgBLliypKXvHO95RWbfqOs5Vp0EPtL699967sm6zdt5557YsN1eO7VqO7d4xZEKXdANwFLCLpDXAfOAoSfsDATwFfLCNbTRrC8e25WbIhB4Rp1cUX9WGtph1lGPbcuMzRc3MMuGEbmaWCSd0M7NM9NQNLr7//e9Xlh955JE1ZQsXLqyse/7559eUNXJX8kZceeWVleXnnHNOW9ZX5bXXXqsp+9znPldZ97LLLqsp67UbA/TqDS4c241zbNdyD93MLBNO6GZmmXBCNzPLhBO6mVkmhnXqf7c8/PDDleVVO45mzJhRWXf8+PE1ZQPd7fyss86qKZs5c2Zl3aq7jTdybeiq60vfcccdlXWvuqr23JeLL764sm7V6dwDnU5+3HHH1ZQde+yxlXU3bNhQWW7D49guOLab4x66mVkmnNDNzDLhhG5mlgkndDOzTDihm5lloqdO/b/gggsqywfaC95tmzdvriy/+eaba8ouv/zymrKlS5u/s1nVKc/nnXde3fPfd999leWnnHJKTVm7TjNvRK+e+u/Ybpxju5Z76GZmmXBCNzPLhBO6mVkmnNDNzDIx5E5RSVOA64DdgTeABRHx95J2Ar4BTKO4me57I2LQCww3u+No6623riy/4ooraspmzZrVzKqA6h0/VacmAyxfvrym7Pbbb6+s+/jjjzfXsAaMGzeupuy2226rrHvMMcfUvdwbb7yxpuyMM86ov2Ft0shOUcf2lhzbhV6O7Xp66JuB8yNiH+Aw4BxJ+wJzgcURMQNYnJ6b9RLHtmVlyIQeEWsj4sfp8UZgGTAJOAm4NlW7Fji5XY00awfHtuWmoastSpoGHAAsAXaLiLVQfDAk7TrAPLOA5n8jmrWRY9tyUHdClzQB+CYwJyI2SPUNVUbEAmBBWkbnzmIyq5Nj23JR11EuksZRBPz1EbEoFT8vaWKaPhFY154mmrWPY9tyUs9RLqIYR3wxIuaUyj8H/DQiLpE0F9gpIv5iiGW1pRczZkzt99LZZ59dWbfqlOPHHnus7nVV3Wm810ydOrWy/M4776wpG+hmClVx8+CDD1bWPfjggxtoXXMaPMrFsV3i2C70cmzXM+RyOPCHwCOSHkpl84BLgJsk/QnwNHDacBtq1iWObcvKkAk9Iv4dGOibof6DO81GGMe25cZnipqZZcIJ3cwsEz11PXRrr6odSosXL66sO3369Jqyp59+uu667dKr10O39hotse0euplZJpzQzcwy4YRuZpYJJ3Qzs0w4oZuZZcJHudig9txzz8ryj3zkIzVlX/jCFyrrrlixoqVtGoyPcrF65Rjb7qGbmWXCCd3MLBNO6GZmmXBCNzPLhHeKWla8U9Ry5Z2iZmajiBO6mVkmnNDNzDLhhG5mlokhE7qkKZLukbRM0mOSzkvlF0l6RtJD6e/E9jfXrHUc25abIY9ykTQRmBgRP5a0HfAAcDLwXuCViLi07pX5SABrs0aOcnFsWy+pJ7bruUn0WmBterxR0jJgUvPNM+sux7blpqExdEnTgAOAJalotqSHJV0taccB5pklaamkpU211KyNHNuWg7pPLJI0Afg34G8jYpGk3YD1QAB/TfHT9ewhluGfpdZWwzmxyLFtvaCe2K4roUsaB3wHuDMiLquYPg34TkTsN8RyHPTWVo0mdMe29YqWnCkqScBVwLJywKcdSn1OAR4dTiPNusWxbbmp5yiXI4AfAI8Ab6TiecDpwP4UP0ufAj6YdjINtiz3YqytGjzKxbFtPaNlQy6t4qC3dvPFuSxXvjiXmdko4oRuZpYJJ3Qzs0w4oZuZZcIJ3cwsE07oZmaZcEI3M8uEE7qZWSaGvHxui60H/ic93iU9z423q3umdnHdfbHdC6/TcOW6bb2wXXXFdkfPFN1ixdLSiDioKytvI2/X6Jbz65TrtuW0XR5yMTPLhBO6mVkmupnQF3Rx3e3k7Rrdcn6dct22bLara2PoZmbWWh5yMTPLRMcTuqQTJD0haYWkuZ1efyulGwivk/RoqWwnSXdJWp7+V95geCSTNEXSPZKWSXpM0nmpvOe3rZ1yiW3Hde9tW5+OJnRJY4EvAe8G9gVOl7RvJ9vQYtcAJ/QrmwssjogZwOL0vNdsBs6PiH2Aw4Bz0vuUw7a1RWaxfQ2O657U6R76IcCKiFgVET8HbgRO6nAbWiYi7gVe7Fd8EnBtenwtcHJHG9UCEbE2In6cHm8ElgGTyGDb2iib2HZc99629el0Qp8ErC49X5PKcrJb3/0n0/9du9yepqS73h8ALCGzbWux3GM7q/c+17judEKvuieeD7MZoSRNAL4JzImIDd1uzwjn2O4ROcd1pxP6GmBK6flk4NkOt6Hdnpc0ESD9X9fl9gyLpHEUQX99RCxKxVlsW5vkHttZvPe5x3WnE/qPgBmSpkt6E/A+4NYOt6HdbgXOTI/PBL7dxbYMiyQBVwHLIuKy0qSe37Y2yj22e/69Hw1x3fETiySdCFwBjAWujoi/7WgDWkjSDcBRFFdrex6YD9wC3AS8FXgaOC0i+u9gGtEkHQH8AHgEeCMVz6MYb+zpbWunXGLbcd1729bHZ4qamWXCZ4qamWXCCd3MLBNO6GZmmXBCNzPLhBO6mVkmnNDNzDLhhG5mlgkndDOzTPw/gs5ovWjwPzYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mask = torch.ones(1, 1, 28, 28)\n",
    "mask[0, 0, 9, 16] = -1\n",
    "oc_sample = sample * mask\n",
    "\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "axes[0].imshow(unnormalize(sample.squeeze()), cmap=\"gray\")\n",
    "axes[0].set_title(f\"Target Number: {sample_label.item()}\")\n",
    "\n",
    "axes[1].imshow(unnormalize(oc_sample.squeeze()), cmap=\"gray\")\n",
    "axes[1].set_title(f\"Occluded image: {sample_label.item()}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question:\n",
    "\n",
    "why `ConvTranspose2d` can be seen as the gradient of `Conv2d` with respect to its input?\n",
    "\n",
    "https://arxiv.org/abs/1603.07285\n",
    "\n",
    "C matrix is \n",
    "\n",
    "https://en.wikipedia.org/wiki/Toeplitz_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy\n",
    "from sympy import Symbol, Function, MatrixSymbol, Matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(H, W, K, x, w):\n",
    "    result = []\n",
    "    for i in range(H-K+1):\n",
    "        for j in range(W-K+1):\n",
    "            result.append(np.sum(x[i:(K+i-1), j:(K+i-1)]*w))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = MatrixSymbol(\"x^{(l)}\", 5, 5)\n",
    "x_output = MatrixSymbol(\"x^{(l+1)}\", 3, 3)\n",
    "w = MatrixSymbol(\"w\", 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution(x, w, K, N):\n",
    "    res = []\n",
    "    for i in range(N-K+1):\n",
    "        for j in range(N-K+1):\n",
    "            a = sum(Matrix(x)[i:(K+i), j:(K+j)].multiply_elementwise(Matrix(w)))\n",
    "            res.append(a)\n",
    "    return Matrix(res).reshape(N-K+1, N-K+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}w_{0, 0} x^{(l)}_{0, 0} + w_{0, 1} x^{(l)}_{0, 1} + w_{0, 2} x^{(l)}_{0, 2} + w_{1, 0} x^{(l)}_{1, 0} + w_{1, 1} x^{(l)}_{1, 1} + w_{1, 2} x^{(l)}_{1, 2} + w_{2, 0} x^{(l)}_{2, 0} + w_{2, 1} x^{(l)}_{2, 1} + w_{2, 2} x^{(l)}_{2, 2} & w_{0, 0} x^{(l)}_{0, 1} + w_{0, 1} x^{(l)}_{0, 2} + w_{0, 2} x^{(l)}_{0, 3} + w_{1, 0} x^{(l)}_{1, 1} + w_{1, 1} x^{(l)}_{1, 2} + w_{1, 2} x^{(l)}_{1, 3} + w_{2, 0} x^{(l)}_{2, 1} + w_{2, 1} x^{(l)}_{2, 2} + w_{2, 2} x^{(l)}_{2, 3} & w_{0, 0} x^{(l)}_{0, 2} + w_{0, 1} x^{(l)}_{0, 3} + w_{0, 2} x^{(l)}_{0, 4} + w_{1, 0} x^{(l)}_{1, 2} + w_{1, 1} x^{(l)}_{1, 3} + w_{1, 2} x^{(l)}_{1, 4} + w_{2, 0} x^{(l)}_{2, 2} + w_{2, 1} x^{(l)}_{2, 3} + w_{2, 2} x^{(l)}_{2, 4}\\\\w_{0, 0} x^{(l)}_{1, 0} + w_{0, 1} x^{(l)}_{1, 1} + w_{0, 2} x^{(l)}_{1, 2} + w_{1, 0} x^{(l)}_{2, 0} + w_{1, 1} x^{(l)}_{2, 1} + w_{1, 2} x^{(l)}_{2, 2} + w_{2, 0} x^{(l)}_{3, 0} + w_{2, 1} x^{(l)}_{3, 1} + w_{2, 2} x^{(l)}_{3, 2} & w_{0, 0} x^{(l)}_{1, 1} + w_{0, 1} x^{(l)}_{1, 2} + w_{0, 2} x^{(l)}_{1, 3} + w_{1, 0} x^{(l)}_{2, 1} + w_{1, 1} x^{(l)}_{2, 2} + w_{1, 2} x^{(l)}_{2, 3} + w_{2, 0} x^{(l)}_{3, 1} + w_{2, 1} x^{(l)}_{3, 2} + w_{2, 2} x^{(l)}_{3, 3} & w_{0, 0} x^{(l)}_{1, 2} + w_{0, 1} x^{(l)}_{1, 3} + w_{0, 2} x^{(l)}_{1, 4} + w_{1, 0} x^{(l)}_{2, 2} + w_{1, 1} x^{(l)}_{2, 3} + w_{1, 2} x^{(l)}_{2, 4} + w_{2, 0} x^{(l)}_{3, 2} + w_{2, 1} x^{(l)}_{3, 3} + w_{2, 2} x^{(l)}_{3, 4}\\\\w_{0, 0} x^{(l)}_{2, 0} + w_{0, 1} x^{(l)}_{2, 1} + w_{0, 2} x^{(l)}_{2, 2} + w_{1, 0} x^{(l)}_{3, 0} + w_{1, 1} x^{(l)}_{3, 1} + w_{1, 2} x^{(l)}_{3, 2} + w_{2, 0} x^{(l)}_{4, 0} + w_{2, 1} x^{(l)}_{4, 1} + w_{2, 2} x^{(l)}_{4, 2} & w_{0, 0} x^{(l)}_{2, 1} + w_{0, 1} x^{(l)}_{2, 2} + w_{0, 2} x^{(l)}_{2, 3} + w_{1, 0} x^{(l)}_{3, 1} + w_{1, 1} x^{(l)}_{3, 2} + w_{1, 2} x^{(l)}_{3, 3} + w_{2, 0} x^{(l)}_{4, 1} + w_{2, 1} x^{(l)}_{4, 2} + w_{2, 2} x^{(l)}_{4, 3} & w_{0, 0} x^{(l)}_{2, 2} + w_{0, 1} x^{(l)}_{2, 3} + w_{0, 2} x^{(l)}_{2, 4} + w_{1, 0} x^{(l)}_{3, 2} + w_{1, 1} x^{(l)}_{3, 3} + w_{1, 2} x^{(l)}_{3, 4} + w_{2, 0} x^{(l)}_{4, 2} + w_{2, 1} x^{(l)}_{4, 3} + w_{2, 2} x^{(l)}_{4, 4}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[w[0, 0]*x^{(l)}[0, 0] + w[0, 1]*x^{(l)}[0, 1] + w[0, 2]*x^{(l)}[0, 2] + w[1, 0]*x^{(l)}[1, 0] + w[1, 1]*x^{(l)}[1, 1] + w[1, 2]*x^{(l)}[1, 2] + w[2, 0]*x^{(l)}[2, 0] + w[2, 1]*x^{(l)}[2, 1] + w[2, 2]*x^{(l)}[2, 2], w[0, 0]*x^{(l)}[0, 1] + w[0, 1]*x^{(l)}[0, 2] + w[0, 2]*x^{(l)}[0, 3] + w[1, 0]*x^{(l)}[1, 1] + w[1, 1]*x^{(l)}[1, 2] + w[1, 2]*x^{(l)}[1, 3] + w[2, 0]*x^{(l)}[2, 1] + w[2, 1]*x^{(l)}[2, 2] + w[2, 2]*x^{(l)}[2, 3], w[0, 0]*x^{(l)}[0, 2] + w[0, 1]*x^{(l)}[0, 3] + w[0, 2]*x^{(l)}[0, 4] + w[1, 0]*x^{(l)}[1, 2] + w[1, 1]*x^{(l)}[1, 3] + w[1, 2]*x^{(l)}[1, 4] + w[2, 0]*x^{(l)}[2, 2] + w[2, 1]*x^{(l)}[2, 3] + w[2, 2]*x^{(l)}[2, 4]],\n",
       "[w[0, 0]*x^{(l)}[1, 0] + w[0, 1]*x^{(l)}[1, 1] + w[0, 2]*x^{(l)}[1, 2] + w[1, 0]*x^{(l)}[2, 0] + w[1, 1]*x^{(l)}[2, 1] + w[1, 2]*x^{(l)}[2, 2] + w[2, 0]*x^{(l)}[3, 0] + w[2, 1]*x^{(l)}[3, 1] + w[2, 2]*x^{(l)}[3, 2], w[0, 0]*x^{(l)}[1, 1] + w[0, 1]*x^{(l)}[1, 2] + w[0, 2]*x^{(l)}[1, 3] + w[1, 0]*x^{(l)}[2, 1] + w[1, 1]*x^{(l)}[2, 2] + w[1, 2]*x^{(l)}[2, 3] + w[2, 0]*x^{(l)}[3, 1] + w[2, 1]*x^{(l)}[3, 2] + w[2, 2]*x^{(l)}[3, 3], w[0, 0]*x^{(l)}[1, 2] + w[0, 1]*x^{(l)}[1, 3] + w[0, 2]*x^{(l)}[1, 4] + w[1, 0]*x^{(l)}[2, 2] + w[1, 1]*x^{(l)}[2, 3] + w[1, 2]*x^{(l)}[2, 4] + w[2, 0]*x^{(l)}[3, 2] + w[2, 1]*x^{(l)}[3, 3] + w[2, 2]*x^{(l)}[3, 4]],\n",
       "[w[0, 0]*x^{(l)}[2, 0] + w[0, 1]*x^{(l)}[2, 1] + w[0, 2]*x^{(l)}[2, 2] + w[1, 0]*x^{(l)}[3, 0] + w[1, 1]*x^{(l)}[3, 1] + w[1, 2]*x^{(l)}[3, 2] + w[2, 0]*x^{(l)}[4, 0] + w[2, 1]*x^{(l)}[4, 1] + w[2, 2]*x^{(l)}[4, 2], w[0, 0]*x^{(l)}[2, 1] + w[0, 1]*x^{(l)}[2, 2] + w[0, 2]*x^{(l)}[2, 3] + w[1, 0]*x^{(l)}[3, 1] + w[1, 1]*x^{(l)}[3, 2] + w[1, 2]*x^{(l)}[3, 3] + w[2, 0]*x^{(l)}[4, 1] + w[2, 1]*x^{(l)}[4, 2] + w[2, 2]*x^{(l)}[4, 3], w[0, 0]*x^{(l)}[2, 2] + w[0, 1]*x^{(l)}[2, 3] + w[0, 2]*x^{(l)}[2, 4] + w[1, 0]*x^{(l)}[3, 2] + w[1, 1]*x^{(l)}[3, 3] + w[1, 2]*x^{(l)}[3, 4] + w[2, 0]*x^{(l)}[4, 2] + w[2, 1]*x^{(l)}[4, 3] + w[2, 2]*x^{(l)}[4, 4]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = convolution(x_input, w, K=3, N=5)\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}w_{0, 0} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\w_{0, 1} & w_{0, 0} & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\w_{0, 2} & w_{0, 1} & w_{0, 0} & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & w_{0, 2} & w_{0, 1} & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & w_{0, 2} & 0 & 0 & 0 & 0 & 0 & 0\\\\w_{1, 0} & 0 & 0 & w_{0, 0} & 0 & 0 & 0 & 0 & 0\\\\w_{1, 1} & w_{1, 0} & 0 & w_{0, 1} & w_{0, 0} & 0 & 0 & 0 & 0\\\\w_{1, 2} & w_{1, 1} & w_{1, 0} & w_{0, 2} & w_{0, 1} & w_{0, 0} & 0 & 0 & 0\\\\0 & w_{1, 2} & w_{1, 1} & 0 & w_{0, 2} & w_{0, 1} & 0 & 0 & 0\\\\0 & 0 & w_{1, 2} & 0 & 0 & w_{0, 2} & 0 & 0 & 0\\\\w_{2, 0} & 0 & 0 & w_{1, 0} & 0 & 0 & w_{0, 0} & 0 & 0\\\\w_{2, 1} & w_{2, 0} & 0 & w_{1, 1} & w_{1, 0} & 0 & w_{0, 1} & w_{0, 0} & 0\\\\w_{2, 2} & w_{2, 1} & w_{2, 0} & w_{1, 2} & w_{1, 1} & w_{1, 0} & w_{0, 2} & w_{0, 1} & w_{0, 0}\\\\0 & w_{2, 2} & w_{2, 1} & 0 & w_{1, 2} & w_{1, 1} & 0 & w_{0, 2} & w_{0, 1}\\\\0 & 0 & w_{2, 2} & 0 & 0 & w_{1, 2} & 0 & 0 & w_{0, 2}\\\\0 & 0 & 0 & w_{2, 0} & 0 & 0 & w_{1, 0} & 0 & 0\\\\0 & 0 & 0 & w_{2, 1} & w_{2, 0} & 0 & w_{1, 1} & w_{1, 0} & 0\\\\0 & 0 & 0 & w_{2, 2} & w_{2, 1} & w_{2, 0} & w_{1, 2} & w_{1, 1} & w_{1, 0}\\\\0 & 0 & 0 & 0 & w_{2, 2} & w_{2, 1} & 0 & w_{1, 2} & w_{1, 1}\\\\0 & 0 & 0 & 0 & 0 & w_{2, 2} & 0 & 0 & w_{1, 2}\\\\0 & 0 & 0 & 0 & 0 & 0 & w_{2, 0} & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & w_{2, 1} & w_{2, 0} & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & w_{2, 2} & w_{2, 1} & w_{2, 0}\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & w_{2, 2} & w_{2, 1}\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & w_{2, 2}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "[[w[0, 0], 0, 0, 0, 0, 0, 0, 0, 0], [w[0, 1], w[0, 0], 0, 0, 0, 0, 0, 0, 0], [w[0, 2], w[0, 1], w[0, 0], 0, 0, 0, 0, 0, 0], [0, w[0, 2], w[0, 1], 0, 0, 0, 0, 0, 0], [0, 0, w[0, 2], 0, 0, 0, 0, 0, 0], [w[1, 0], 0, 0, w[0, 0], 0, 0, 0, 0, 0], [w[1, 1], w[1, 0], 0, w[0, 1], w[0, 0], 0, 0, 0, 0], [w[1, 2], w[1, 1], w[1, 0], w[0, 2], w[0, 1], w[0, 0], 0, 0, 0], [0, w[1, 2], w[1, 1], 0, w[0, 2], w[0, 1], 0, 0, 0], [0, 0, w[1, 2], 0, 0, w[0, 2], 0, 0, 0], [w[2, 0], 0, 0, w[1, 0], 0, 0, w[0, 0], 0, 0], [w[2, 1], w[2, 0], 0, w[1, 1], w[1, 0], 0, w[0, 1], w[0, 0], 0], [w[2, 2], w[2, 1], w[2, 0], w[1, 2], w[1, 1], w[1, 0], w[0, 2], w[0, 1], w[0, 0]], [0, w[2, 2], w[2, 1], 0, w[1, 2], w[1, 1], 0, w[0, 2], w[0, 1]], [0, 0, w[2, 2], 0, 0, w[1, 2], 0, 0, w[0, 2]], [0, 0, 0, w[2, 0], 0, 0, w[1, 0], 0, 0], [0, 0, 0, w[2, 1], w[2, 0], 0, w[1, 1], w[1, 0], 0], [0, 0, 0, w[2, 2], w[2, 1], w[2, 0], w[1, 2], w[1, 1], w[1, 0]], [0, 0, 0, 0, w[2, 2], w[2, 1], 0, w[1, 2], w[1, 1]], [0, 0, 0, 0, 0, w[2, 2], 0, 0, w[1, 2]], [0, 0, 0, 0, 0, 0, w[2, 0], 0, 0], [0, 0, 0, 0, 0, 0, w[2, 1], w[2, 0], 0], [0, 0, 0, 0, 0, 0, w[2, 2], w[2, 1], w[2, 0]], [0, 0, 0, 0, 0, 0, 0, w[2, 2], w[2, 1]], [0, 0, 0, 0, 0, 0, 0, 0, w[2, 2]]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = o.reshape(1, 9).diff(Matrix(x_input).reshape(25, 1)).reshape(25, 9)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}w_{0, 0} x^{(l)}_{0, 0} + w_{0, 1} x^{(l)}_{0, 1} + w_{0, 2} x^{(l)}_{0, 2} + w_{1, 0} x^{(l)}_{1, 0} + w_{1, 1} x^{(l)}_{1, 1} + w_{1, 2} x^{(l)}_{1, 2} + w_{2, 0} x^{(l)}_{2, 0} + w_{2, 1} x^{(l)}_{2, 1} + w_{2, 2} x^{(l)}_{2, 2} & w_{0, 0} x^{(l)}_{0, 1} + w_{0, 1} x^{(l)}_{0, 2} + w_{0, 2} x^{(l)}_{0, 3} + w_{1, 0} x^{(l)}_{1, 1} + w_{1, 1} x^{(l)}_{1, 2} + w_{1, 2} x^{(l)}_{1, 3} + w_{2, 0} x^{(l)}_{2, 1} + w_{2, 1} x^{(l)}_{2, 2} + w_{2, 2} x^{(l)}_{2, 3} & w_{0, 0} x^{(l)}_{0, 2} + w_{0, 1} x^{(l)}_{0, 3} + w_{0, 2} x^{(l)}_{0, 4} + w_{1, 0} x^{(l)}_{1, 2} + w_{1, 1} x^{(l)}_{1, 3} + w_{1, 2} x^{(l)}_{1, 4} + w_{2, 0} x^{(l)}_{2, 2} + w_{2, 1} x^{(l)}_{2, 3} + w_{2, 2} x^{(l)}_{2, 4}\\\\w_{0, 0} x^{(l)}_{1, 0} + w_{0, 1} x^{(l)}_{1, 1} + w_{0, 2} x^{(l)}_{1, 2} + w_{1, 0} x^{(l)}_{2, 0} + w_{1, 1} x^{(l)}_{2, 1} + w_{1, 2} x^{(l)}_{2, 2} + w_{2, 0} x^{(l)}_{3, 0} + w_{2, 1} x^{(l)}_{3, 1} + w_{2, 2} x^{(l)}_{3, 2} & w_{0, 0} x^{(l)}_{1, 1} + w_{0, 1} x^{(l)}_{1, 2} + w_{0, 2} x^{(l)}_{1, 3} + w_{1, 0} x^{(l)}_{2, 1} + w_{1, 1} x^{(l)}_{2, 2} + w_{1, 2} x^{(l)}_{2, 3} + w_{2, 0} x^{(l)}_{3, 1} + w_{2, 1} x^{(l)}_{3, 2} + w_{2, 2} x^{(l)}_{3, 3} & w_{0, 0} x^{(l)}_{1, 2} + w_{0, 1} x^{(l)}_{1, 3} + w_{0, 2} x^{(l)}_{1, 4} + w_{1, 0} x^{(l)}_{2, 2} + w_{1, 1} x^{(l)}_{2, 3} + w_{1, 2} x^{(l)}_{2, 4} + w_{2, 0} x^{(l)}_{3, 2} + w_{2, 1} x^{(l)}_{3, 3} + w_{2, 2} x^{(l)}_{3, 4}\\\\w_{0, 0} x^{(l)}_{2, 0} + w_{0, 1} x^{(l)}_{2, 1} + w_{0, 2} x^{(l)}_{2, 2} + w_{1, 0} x^{(l)}_{3, 0} + w_{1, 1} x^{(l)}_{3, 1} + w_{1, 2} x^{(l)}_{3, 2} + w_{2, 0} x^{(l)}_{4, 0} + w_{2, 1} x^{(l)}_{4, 1} + w_{2, 2} x^{(l)}_{4, 2} & w_{0, 0} x^{(l)}_{2, 1} + w_{0, 1} x^{(l)}_{2, 2} + w_{0, 2} x^{(l)}_{2, 3} + w_{1, 0} x^{(l)}_{3, 1} + w_{1, 1} x^{(l)}_{3, 2} + w_{1, 2} x^{(l)}_{3, 3} + w_{2, 0} x^{(l)}_{4, 1} + w_{2, 1} x^{(l)}_{4, 2} + w_{2, 2} x^{(l)}_{4, 3} & w_{0, 0} x^{(l)}_{2, 2} + w_{0, 1} x^{(l)}_{2, 3} + w_{0, 2} x^{(l)}_{2, 4} + w_{1, 0} x^{(l)}_{3, 2} + w_{1, 1} x^{(l)}_{3, 3} + w_{1, 2} x^{(l)}_{3, 4} + w_{2, 0} x^{(l)}_{4, 2} + w_{2, 1} x^{(l)}_{4, 3} + w_{2, 2} x^{(l)}_{4, 4}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[w[0, 0]*x^{(l)}[0, 0] + w[0, 1]*x^{(l)}[0, 1] + w[0, 2]*x^{(l)}[0, 2] + w[1, 0]*x^{(l)}[1, 0] + w[1, 1]*x^{(l)}[1, 1] + w[1, 2]*x^{(l)}[1, 2] + w[2, 0]*x^{(l)}[2, 0] + w[2, 1]*x^{(l)}[2, 1] + w[2, 2]*x^{(l)}[2, 2], w[0, 0]*x^{(l)}[0, 1] + w[0, 1]*x^{(l)}[0, 2] + w[0, 2]*x^{(l)}[0, 3] + w[1, 0]*x^{(l)}[1, 1] + w[1, 1]*x^{(l)}[1, 2] + w[1, 2]*x^{(l)}[1, 3] + w[2, 0]*x^{(l)}[2, 1] + w[2, 1]*x^{(l)}[2, 2] + w[2, 2]*x^{(l)}[2, 3], w[0, 0]*x^{(l)}[0, 2] + w[0, 1]*x^{(l)}[0, 3] + w[0, 2]*x^{(l)}[0, 4] + w[1, 0]*x^{(l)}[1, 2] + w[1, 1]*x^{(l)}[1, 3] + w[1, 2]*x^{(l)}[1, 4] + w[2, 0]*x^{(l)}[2, 2] + w[2, 1]*x^{(l)}[2, 3] + w[2, 2]*x^{(l)}[2, 4]],\n",
       "[w[0, 0]*x^{(l)}[1, 0] + w[0, 1]*x^{(l)}[1, 1] + w[0, 2]*x^{(l)}[1, 2] + w[1, 0]*x^{(l)}[2, 0] + w[1, 1]*x^{(l)}[2, 1] + w[1, 2]*x^{(l)}[2, 2] + w[2, 0]*x^{(l)}[3, 0] + w[2, 1]*x^{(l)}[3, 1] + w[2, 2]*x^{(l)}[3, 2], w[0, 0]*x^{(l)}[1, 1] + w[0, 1]*x^{(l)}[1, 2] + w[0, 2]*x^{(l)}[1, 3] + w[1, 0]*x^{(l)}[2, 1] + w[1, 1]*x^{(l)}[2, 2] + w[1, 2]*x^{(l)}[2, 3] + w[2, 0]*x^{(l)}[3, 1] + w[2, 1]*x^{(l)}[3, 2] + w[2, 2]*x^{(l)}[3, 3], w[0, 0]*x^{(l)}[1, 2] + w[0, 1]*x^{(l)}[1, 3] + w[0, 2]*x^{(l)}[1, 4] + w[1, 0]*x^{(l)}[2, 2] + w[1, 1]*x^{(l)}[2, 3] + w[1, 2]*x^{(l)}[2, 4] + w[2, 0]*x^{(l)}[3, 2] + w[2, 1]*x^{(l)}[3, 3] + w[2, 2]*x^{(l)}[3, 4]],\n",
       "[w[0, 0]*x^{(l)}[2, 0] + w[0, 1]*x^{(l)}[2, 1] + w[0, 2]*x^{(l)}[2, 2] + w[1, 0]*x^{(l)}[3, 0] + w[1, 1]*x^{(l)}[3, 1] + w[1, 2]*x^{(l)}[3, 2] + w[2, 0]*x^{(l)}[4, 0] + w[2, 1]*x^{(l)}[4, 1] + w[2, 2]*x^{(l)}[4, 2], w[0, 0]*x^{(l)}[2, 1] + w[0, 1]*x^{(l)}[2, 2] + w[0, 2]*x^{(l)}[2, 3] + w[1, 0]*x^{(l)}[3, 1] + w[1, 1]*x^{(l)}[3, 2] + w[1, 2]*x^{(l)}[3, 3] + w[2, 0]*x^{(l)}[4, 1] + w[2, 1]*x^{(l)}[4, 2] + w[2, 2]*x^{(l)}[4, 3], w[0, 0]*x^{(l)}[2, 2] + w[0, 1]*x^{(l)}[2, 3] + w[0, 2]*x^{(l)}[2, 4] + w[1, 0]*x^{(l)}[3, 2] + w[1, 1]*x^{(l)}[3, 3] + w[1, 2]*x^{(l)}[3, 4] + w[2, 0]*x^{(l)}[4, 2] + w[2, 1]*x^{(l)}[4, 3] + w[2, 2]*x^{(l)}[4, 4]]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward \n",
    "(Matrix(x_input).reshape(1, 25) @ C).reshape(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}w_{0, 0} x^{(l+1)}_{0, 0} & w_{0, 0} x^{(l+1)}_{0, 1} + w_{0, 1} x^{(l+1)}_{0, 0} & w_{0, 0} x^{(l+1)}_{0, 2} + w_{0, 1} x^{(l+1)}_{0, 1} + w_{0, 2} x^{(l+1)}_{0, 0} & w_{0, 1} x^{(l+1)}_{0, 2} + w_{0, 2} x^{(l+1)}_{0, 1} & w_{0, 2} x^{(l+1)}_{0, 2}\\\\w_{0, 0} x^{(l+1)}_{1, 0} + w_{1, 0} x^{(l+1)}_{0, 0} & w_{0, 0} x^{(l+1)}_{1, 1} + w_{0, 1} x^{(l+1)}_{1, 0} + w_{1, 0} x^{(l+1)}_{0, 1} + w_{1, 1} x^{(l+1)}_{0, 0} & w_{0, 0} x^{(l+1)}_{1, 2} + w_{0, 1} x^{(l+1)}_{1, 1} + w_{0, 2} x^{(l+1)}_{1, 0} + w_{1, 0} x^{(l+1)}_{0, 2} + w_{1, 1} x^{(l+1)}_{0, 1} + w_{1, 2} x^{(l+1)}_{0, 0} & w_{0, 1} x^{(l+1)}_{1, 2} + w_{0, 2} x^{(l+1)}_{1, 1} + w_{1, 1} x^{(l+1)}_{0, 2} + w_{1, 2} x^{(l+1)}_{0, 1} & w_{0, 2} x^{(l+1)}_{1, 2} + w_{1, 2} x^{(l+1)}_{0, 2}\\\\w_{0, 0} x^{(l+1)}_{2, 0} + w_{1, 0} x^{(l+1)}_{1, 0} + w_{2, 0} x^{(l+1)}_{0, 0} & w_{0, 0} x^{(l+1)}_{2, 1} + w_{0, 1} x^{(l+1)}_{2, 0} + w_{1, 0} x^{(l+1)}_{1, 1} + w_{1, 1} x^{(l+1)}_{1, 0} + w_{2, 0} x^{(l+1)}_{0, 1} + w_{2, 1} x^{(l+1)}_{0, 0} & w_{0, 0} x^{(l+1)}_{2, 2} + w_{0, 1} x^{(l+1)}_{2, 1} + w_{0, 2} x^{(l+1)}_{2, 0} + w_{1, 0} x^{(l+1)}_{1, 2} + w_{1, 1} x^{(l+1)}_{1, 1} + w_{1, 2} x^{(l+1)}_{1, 0} + w_{2, 0} x^{(l+1)}_{0, 2} + w_{2, 1} x^{(l+1)}_{0, 1} + w_{2, 2} x^{(l+1)}_{0, 0} & w_{0, 1} x^{(l+1)}_{2, 2} + w_{0, 2} x^{(l+1)}_{2, 1} + w_{1, 1} x^{(l+1)}_{1, 2} + w_{1, 2} x^{(l+1)}_{1, 1} + w_{2, 1} x^{(l+1)}_{0, 2} + w_{2, 2} x^{(l+1)}_{0, 1} & w_{0, 2} x^{(l+1)}_{2, 2} + w_{1, 2} x^{(l+1)}_{1, 2} + w_{2, 2} x^{(l+1)}_{0, 2}\\\\w_{1, 0} x^{(l+1)}_{2, 0} + w_{2, 0} x^{(l+1)}_{1, 0} & w_{1, 0} x^{(l+1)}_{2, 1} + w_{1, 1} x^{(l+1)}_{2, 0} + w_{2, 0} x^{(l+1)}_{1, 1} + w_{2, 1} x^{(l+1)}_{1, 0} & w_{1, 0} x^{(l+1)}_{2, 2} + w_{1, 1} x^{(l+1)}_{2, 1} + w_{1, 2} x^{(l+1)}_{2, 0} + w_{2, 0} x^{(l+1)}_{1, 2} + w_{2, 1} x^{(l+1)}_{1, 1} + w_{2, 2} x^{(l+1)}_{1, 0} & w_{1, 1} x^{(l+1)}_{2, 2} + w_{1, 2} x^{(l+1)}_{2, 1} + w_{2, 1} x^{(l+1)}_{1, 2} + w_{2, 2} x^{(l+1)}_{1, 1} & w_{1, 2} x^{(l+1)}_{2, 2} + w_{2, 2} x^{(l+1)}_{1, 2}\\\\w_{2, 0} x^{(l+1)}_{2, 0} & w_{2, 0} x^{(l+1)}_{2, 1} + w_{2, 1} x^{(l+1)}_{2, 0} & w_{2, 0} x^{(l+1)}_{2, 2} + w_{2, 1} x^{(l+1)}_{2, 1} + w_{2, 2} x^{(l+1)}_{2, 0} & w_{2, 1} x^{(l+1)}_{2, 2} + w_{2, 2} x^{(l+1)}_{2, 1} & w_{2, 2} x^{(l+1)}_{2, 2}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[                                                    w[0, 0]*x^{(l+1)}[0, 0],                                                                                                         w[0, 0]*x^{(l+1)}[0, 1] + w[0, 1]*x^{(l+1)}[0, 0],                                                                                                                                                             w[0, 0]*x^{(l+1)}[0, 2] + w[0, 1]*x^{(l+1)}[0, 1] + w[0, 2]*x^{(l+1)}[0, 0],                                                                                                         w[0, 1]*x^{(l+1)}[0, 2] + w[0, 2]*x^{(l+1)}[0, 1],                                                     w[0, 2]*x^{(l+1)}[0, 2]],\n",
       "[                          w[0, 0]*x^{(l+1)}[1, 0] + w[1, 0]*x^{(l+1)}[0, 0],                                                     w[0, 0]*x^{(l+1)}[1, 1] + w[0, 1]*x^{(l+1)}[1, 0] + w[1, 0]*x^{(l+1)}[0, 1] + w[1, 1]*x^{(l+1)}[0, 0],                                                                               w[0, 0]*x^{(l+1)}[1, 2] + w[0, 1]*x^{(l+1)}[1, 1] + w[0, 2]*x^{(l+1)}[1, 0] + w[1, 0]*x^{(l+1)}[0, 2] + w[1, 1]*x^{(l+1)}[0, 1] + w[1, 2]*x^{(l+1)}[0, 0],                                                     w[0, 1]*x^{(l+1)}[1, 2] + w[0, 2]*x^{(l+1)}[1, 1] + w[1, 1]*x^{(l+1)}[0, 2] + w[1, 2]*x^{(l+1)}[0, 1],                           w[0, 2]*x^{(l+1)}[1, 2] + w[1, 2]*x^{(l+1)}[0, 2]],\n",
       "[w[0, 0]*x^{(l+1)}[2, 0] + w[1, 0]*x^{(l+1)}[1, 0] + w[2, 0]*x^{(l+1)}[0, 0], w[0, 0]*x^{(l+1)}[2, 1] + w[0, 1]*x^{(l+1)}[2, 0] + w[1, 0]*x^{(l+1)}[1, 1] + w[1, 1]*x^{(l+1)}[1, 0] + w[2, 0]*x^{(l+1)}[0, 1] + w[2, 1]*x^{(l+1)}[0, 0], w[0, 0]*x^{(l+1)}[2, 2] + w[0, 1]*x^{(l+1)}[2, 1] + w[0, 2]*x^{(l+1)}[2, 0] + w[1, 0]*x^{(l+1)}[1, 2] + w[1, 1]*x^{(l+1)}[1, 1] + w[1, 2]*x^{(l+1)}[1, 0] + w[2, 0]*x^{(l+1)}[0, 2] + w[2, 1]*x^{(l+1)}[0, 1] + w[2, 2]*x^{(l+1)}[0, 0], w[0, 1]*x^{(l+1)}[2, 2] + w[0, 2]*x^{(l+1)}[2, 1] + w[1, 1]*x^{(l+1)}[1, 2] + w[1, 2]*x^{(l+1)}[1, 1] + w[2, 1]*x^{(l+1)}[0, 2] + w[2, 2]*x^{(l+1)}[0, 1], w[0, 2]*x^{(l+1)}[2, 2] + w[1, 2]*x^{(l+1)}[1, 2] + w[2, 2]*x^{(l+1)}[0, 2]],\n",
       "[                          w[1, 0]*x^{(l+1)}[2, 0] + w[2, 0]*x^{(l+1)}[1, 0],                                                     w[1, 0]*x^{(l+1)}[2, 1] + w[1, 1]*x^{(l+1)}[2, 0] + w[2, 0]*x^{(l+1)}[1, 1] + w[2, 1]*x^{(l+1)}[1, 0],                                                                               w[1, 0]*x^{(l+1)}[2, 2] + w[1, 1]*x^{(l+1)}[2, 1] + w[1, 2]*x^{(l+1)}[2, 0] + w[2, 0]*x^{(l+1)}[1, 2] + w[2, 1]*x^{(l+1)}[1, 1] + w[2, 2]*x^{(l+1)}[1, 0],                                                     w[1, 1]*x^{(l+1)}[2, 2] + w[1, 2]*x^{(l+1)}[2, 1] + w[2, 1]*x^{(l+1)}[1, 2] + w[2, 2]*x^{(l+1)}[1, 1],                           w[1, 2]*x^{(l+1)}[2, 2] + w[2, 2]*x^{(l+1)}[1, 2]],\n",
       "[                                                    w[2, 0]*x^{(l+1)}[2, 0],                                                                                                         w[2, 0]*x^{(l+1)}[2, 1] + w[2, 1]*x^{(l+1)}[2, 0],                                                                                                                                                             w[2, 0]*x^{(l+1)}[2, 2] + w[2, 1]*x^{(l+1)}[2, 1] + w[2, 2]*x^{(l+1)}[2, 0],                                                                                                         w[2, 1]*x^{(l+1)}[2, 2] + w[2, 2]*x^{(l+1)}[2, 1],                                                     w[2, 2]*x^{(l+1)}[2, 2]]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# backward\n",
    "(Matrix(x_output).reshape(1, 9) @ C.transpose()).reshape(5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
