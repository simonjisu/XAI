{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "class XaiHook(object):\n",
    "    def __init__(self, module, backward=False, hook_fn=None):\n",
    "        \"\"\"\n",
    "        Hook Handler Module\n",
    "        \"\"\"\n",
    "        self.module = module\n",
    "        self.backward = backward\n",
    "        if hook_fn is None:\n",
    "            self.hook_fn = self.default_hook_fn\n",
    "        else:\n",
    "            self.hook_fn = hook_fn\n",
    "        \n",
    "    def register_hook(self):\n",
    "        \"\"\"\n",
    "        defalut hook_function is save (module, input, output) to (m, i, o)\n",
    "        if you want to use hook function, change `hook_function` \n",
    "        \"\"\"\n",
    "        if not self.backward:\n",
    "            self.hook = self.module.register_forward_hook(self.hook_fn)\n",
    "        else:\n",
    "            self.hook = self.module.register_backward_hook(self.hook_fn)\n",
    "            \n",
    "    def default_hook_fn(self, m, i, o):\n",
    "        \"\"\"\n",
    "        forward\n",
    "         - m: module class\n",
    "         - i: forward input from previous layer\n",
    "         - o: forward output to next layer\n",
    "        backward\n",
    "         - m: module class\n",
    "         - i: gradient input to next layer (backward out)\n",
    "         - o: gradient output from previous layer (backward in)\n",
    "        \"\"\"\n",
    "        self.m = m\n",
    "        self.i = i\n",
    "        self.o = o\n",
    "        \n",
    "    def close(self):\n",
    "        self.hook.remove()\n",
    "        self.m = None\n",
    "        self.i = None\n",
    "        self.o = None\n",
    "\n",
    "\n",
    "class XaiBase(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XaiBase, self).__init__()\n",
    "        \"\"\"\n",
    "        need to define hook function at each method\n",
    "        - f_hook\n",
    "        - b_hook\n",
    "        \"\"\"\n",
    "        self._reset_maps()\n",
    "    \n",
    "    def _reset_maps(self):\n",
    "        self.maps = OrderedDict()\n",
    "        \n",
    "    def _save_maps(self, layer_name, x):\n",
    "        self.maps[layer_name] = x    \n",
    "        \n",
    "    def _register(self, hooks):\n",
    "        \"\"\"\n",
    "        - need to define XaiHook class to use\n",
    "        - defalut hook_function is save (module, input, output) to (m, i, o)\n",
    "          if you want to use hook function, change `hook_function` \n",
    "        \"\"\"\n",
    "        if not isinstance(hooks, list):\n",
    "            hooks = [hooks]\n",
    "        for hook in hooks:\n",
    "            hook.register_hook()\n",
    "            \n",
    "    def _reset_hooks(self, hooks):\n",
    "        if not isinstance(hooks, list):\n",
    "            hooks = [hooks]\n",
    "        for hook in hooks:\n",
    "            hook.close()\n",
    "\n",
    "    def _return_indices(self, layers, on=True):\n",
    "        \"\"\"\n",
    "        support for cnn layer which have `nn.MaxPool2d`,\n",
    "        you can turn on/off pooling indices.\n",
    "        please define a forward function to use it in your model\n",
    "        '''\n",
    "        # in your model\n",
    "        def forward_switch(self, x):\n",
    "            switches = OrderedDict()\n",
    "            self.return_indices(on=True)\n",
    "            for idx, layer in enumerate(self.convs):\n",
    "                if isinstance(layer, nn.MaxPool2d):\n",
    "                    x, indices = layer(x)\n",
    "                    switches[idx] = indices\n",
    "                else:\n",
    "                    x = layer(x)\n",
    "            self.return_indices(on=False)\n",
    "            return x, switches\n",
    "        '''\n",
    "        \"\"\"\n",
    "        if on:\n",
    "            for layer in layers:\n",
    "                if isinstance(layer, nn.MaxPool2d):\n",
    "                    layer.return_indices = True\n",
    "        else:\n",
    "            for layer in layers:\n",
    "                if isinstance(layer, nn.MaxPool2d):\n",
    "                    layer.return_indices = False  \n",
    "                    \n",
    "                    \n",
    "class XaiModel(XaiBase):\n",
    "    def __init__(self, model):\n",
    "        super(XaiModel, self).__init__()\n",
    "        self.model = deepcopy(model)\n",
    "        self.model.eval()\n",
    "        \n",
    "    def one_hot(self, targets, module_name):\n",
    "        \"\"\"\n",
    "        one hot vectorize the target tensor\n",
    "        args:\n",
    "        - targets: torch.LongTensor, target classes that have size of mini-batch\n",
    "        - module_name: str, feature name for Fully-Connected Network or any Task-specific Network\n",
    "        return:\n",
    "        - one hot vector of targets\n",
    "        \"\"\"\n",
    "        assert isinstance(targets, torch.LongTensor), \"`targets` must be `torch.LongTensor` type\"\n",
    "        assert isinstance(module_name, str), \"`module_name` must be `str` type\"\n",
    "        modules = self.model._modules[module_name]\n",
    "        if isinstance(modules, nn.Sequential):\n",
    "            last_layer = modules[-1]\n",
    "        else:\n",
    "            last_layer = modules\n",
    "        assert isinstance(last_layer, nn.Linear), \"`last layer` must be `torch.nn.Linear`\"\n",
    "        target_size = self.model._modules[module_name][-1].out_features\n",
    "        B = targets.size(0)\n",
    "        one_hot = torch.zeros((B, target_size))\n",
    "        one_hot.scatter_(1, targets.unsqueeze(1), 1.0)\n",
    "        return one_hot.to(targets.device)\n",
    "    \n",
    "    def find_target_layer_idx(self, module_name, layer_names):\n",
    "        assert isinstance(layer_names, list), \"use list for `layer_names`\"\n",
    "        layer_names = [l.lower() for l in layer_names]\n",
    "        idxes = defaultdict(list)\n",
    "        modules = self.model._modules[module_name]\n",
    "        assert isinstance(modules, nn.Sequential), \"use this function for `nn.Sequential` type modules\"\n",
    "        for idx, layer in modules.named_children():\n",
    "            l_name = type(layer).__name__.lower()\n",
    "            if l_name in layer_names:\n",
    "                idxes[l_name].append(int(idx))\n",
    "\n",
    "        return idxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(nn.Module):\n",
    "    \"\"\"Channel Attention Module\"\"\"\n",
    "    def __init__(self, C, ratio):\n",
    "        \"\"\"\n",
    "        Method in [arXiv:1807.06521]\n",
    "        args:\n",
    "         - C: channel of input features\n",
    "         - H: height of input features\n",
    "         - W: width of input features\n",
    "         - hid_size: hidden size of shallow network\n",
    "         - ratio: reduction ratio\n",
    "        \"\"\"\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        assert isinstance(2*C // ratio, int), \"`2*C // ratio` must be int \"\n",
    "        self.maxpool = nn.AdaptiveMaxPool2d((1, 1))\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.shallow_net = nn.Sequential(\n",
    "            nn.Linear(2*C, 2*C // ratio),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2*C // ratio, 2*C),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # (B, C, H, W) > (B, 2*C, 1, 1)\n",
    "        x = torch.cat([self.maxpool(x), self.avgpool(x)], dim=1)\n",
    "        # (B, 2*C) > (B, 2*C//2) > (B, 2*C)\n",
    "        x = self.shallow_net(x.squeeze(-1).squeeze(-1))\n",
    "        # (B, C), (B, C)\n",
    "        x_max, x_avg = torch.chunk(x, 2, dim=1)\n",
    "        # not using softmax in paper: something like gate function\n",
    "        x = torch.sigmoid(x_max + x_avg)\n",
    "        return x.unsqueeze(-1).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAttention(nn.Module):\n",
    "    \"\"\"Spatial Attention Module\"\"\"\n",
    "    def __init__(self, H, W, K_H=7, K_W=7, S_H=1, S_W=1):\n",
    "        \"\"\"\n",
    "        Method in [arXiv:1807.06521]\n",
    "        args:\n",
    "         - H: height of input features\n",
    "         - W: width of input features\n",
    "         - K_H: height of kernel size\n",
    "         - K_W: width of kernel size\n",
    "         - S_H: stride height of conv layer\n",
    "         - S_W: stride width of conv layer\n",
    "        \"\"\"\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        P_H = self.cal_padding_size(H, K_H, S_H)\n",
    "        P_W = self.cal_padding_size(W, K_W, S_W)\n",
    "        kernel_size = (K_H, K_W)\n",
    "        stride = (S_H, S_W)\n",
    "        padding = (P_H, P_W)\n",
    "        # same padding conv layer\n",
    "        self.conv_layer = nn.Conv2d(2, 1, kernel_size, stride, padding)\n",
    "    \n",
    "    def cal_padding_size(self, x, K, S):\n",
    "        return int((S * (x-1) + K - x) / 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # (B, C, H, W) > (B, 1, H, W)\n",
    "        x_max, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x_avg = torch.mean(x, dim=1, keepdim=True)\n",
    "        # (B, 2, H, W)\n",
    "        x = torch.cat([x_max, x_avg], dim=1)\n",
    "        # (B, 2, H, W) > (B, 1, H, W)\n",
    "        x = self.conv_layer(x)\n",
    "        # return gated features\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBAM(nn.Module):\n",
    "    \"\"\"Convolution Block Attention Module\"\"\"\n",
    "    def __init__(self, C, H, W, ratio, K_H=7, K_W=7, S_H=1, S_W=1):\n",
    "        \"\"\"\n",
    "        Method in [arXiv:1807.06521]\n",
    "        args:\n",
    "         - C: channel of input features\n",
    "         - H: height of input features\n",
    "         - W: width of input features\n",
    "         - ratio: reduction ratio\n",
    "         - K_H: height of kernel size\n",
    "         - K_W: width of kernel size\n",
    "         - S_H: stride height of conv layer\n",
    "         - S_W: stride width of conv layer\n",
    "         \n",
    "        return:\n",
    "         - attentioned features, size = (B, C, H, W)\n",
    "        \"\"\"\n",
    "        super(CBAM, self).__init__()\n",
    "        self.channel_attn = ChannelAttention(C, ratio)\n",
    "        self.spatial_attn = SpatialAttention(H, W, K_H, K_W, S_H, S_W)\n",
    "        \n",
    "    def forward(self, x, return_attn=False):\n",
    "        \"\"\"\n",
    "        return: attentioned features, size = (B, C, H, W)\n",
    "        \"\"\"\n",
    "        out = x\n",
    "        c_attn = self.channel_attn(out)\n",
    "        out = c_attn * out\n",
    "        s_attn = self.spatial_attn(out)\n",
    "        out = s_attn * out\n",
    "        if return_attn:\n",
    "            return out, (c_attn, s_attn)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "class CnnWithCBAM(XaiBase):\n",
    "    def __init__(self):\n",
    "        super(CnnWithCBAM, self).__init__()\n",
    "\n",
    "        self.convs = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 5),  # (B, 1, 28, 28) > (B, 32, 24, 24)\n",
    "            CBAM(32, 24, 24, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # (B, 32, 24, 24) > (B, 32, 12, 12)\n",
    "            nn.Conv2d(32, 64, 3),  # (B, 32, 12, 12) > (B, 64, 10, 10)\n",
    "            CBAM(64, 10, 10, 16),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(2),  # (B, 64, 10, 10) > (B, 64, 5, 5)\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64*5*5, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        x = self.convs(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def forward_map(self, x):\n",
    "        self._reset_maps()\n",
    "        for i, layer in enumerate(self.convs):\n",
    "            layer_name = type(layer).__name__\n",
    "            if layer_name == \"CBAM\":\n",
    "                x, attns = layer(x, return_attn=True)\n",
    "                self._save_maps(f\"{i}\"+layer_name, attns)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CnnWithCBAM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class GradCAM(XaiModel):\n",
    "    def __init__(self, model, norm_mode=1):\n",
    "        \"\"\"\n",
    "        norm mode\n",
    "        - 1 ( 0, 1) min-max normalization\n",
    "        - 2 (-1, 1) min-max normalization\n",
    "        - 3 mean-std normalization\n",
    "        \"\"\"\n",
    "        super(GradCAM, self).__init__(model)\n",
    "        self.norm_mode = norm_mode\n",
    "        self.global_avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        relu_idxes = self.find_target_layer_idx(module_name=\"convs\", layer_names=[\"relu\"])\n",
    "        self.last_relu_idx = relu_idxes[\"relu\"][-1]\n",
    "        # get Rectified Conv Features Maps\n",
    "        self.f_hook = XaiHook(self.model.convs[self.last_relu_idx])\n",
    "        self.b_hook = XaiHook(self.model.convs[self.last_relu_idx], backward=True)\n",
    "        self.hooks = [self.f_hook, self.b_hook]\n",
    "    \n",
    "    def cal_gradcam(self):\n",
    "        # (B, C, H, W) > (B, C, 1, 1)\n",
    "        alpha = self.global_avgpool(self.b_hook.i[0])\n",
    "        # sum( (B, C, 1, 1) * (B, C, H, W) , dim=1) > (B, 1, H, W)\n",
    "        gradcam = torch.relu((alpha * self.f_hook.o).sum(1, keepdim=True))\n",
    "        return gradcam\n",
    "        \n",
    "    def post_processing(self, gradcam, H, W):\n",
    "        \"\"\"\n",
    "        interpolate(up sample) & normalize\n",
    "        https://pytorch.org/docs/stable/nn.functional.html#interpolate\n",
    "        \"\"\"\n",
    "        gradcam = F.interpolate(gradcam, size=(H, W), mode=\"bilinear\", align_corners=False)\n",
    "        gradcam = self.normalization(gradcam)\n",
    "        return gradcam\n",
    "    \n",
    "    def normalization(self, tensor):\n",
    "        B, C, H, W = tensor.size()\n",
    "        tensor = tensor.view(B, -1)\n",
    "        t_min = tensor.min(dim=1, keepdim=True)[0]\n",
    "        t_max = tensor.max(dim=1, keepdim=True)[0]\n",
    "        t_mean = tensor.mean(dim=1, keepdim=True)\n",
    "        t_std = tensor.std(dim=1, keepdim=True)\n",
    "        if self.norm_mode == 1:\n",
    "            tensor -= t_min\n",
    "            tensor /= (t_max - t_min)\n",
    "        elif self.norm_mode == 2:\n",
    "            tensor -= t_min\n",
    "            tensor *= 2\n",
    "            tensor /= (t_max - t_min)\n",
    "        elif self.norm_mode == 3:\n",
    "            tensor -= t_mean\n",
    "            tensor /= t_std\n",
    "        return tensor.view(B, C, H, W)\n",
    "        \n",
    "    def get_attribution(self, x, targets):\n",
    "        *_, H, W = x.size()\n",
    "        self._register(self.hooks)\n",
    "        self.model.zero_grad()\n",
    "        \n",
    "        output = self.model(x)\n",
    "        grad = self.one_hot(targets, module_name=\"fc\")\n",
    "        output.backward(grad)\n",
    "        \n",
    "        gradcam = self.cal_gradcam()\n",
    "        gradcam = self.post_processing(gradcam, H, W)\n",
    "        \n",
    "        self._reset_hooks(self.hooks)\n",
    "        return gradcam.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gradcam_model = GradCAM(model)\n",
    "x = torch.rand(1, 1, 28, 28)\n",
    "targets = torch.LongTensor([2])\n",
    "gradcam = gradcam_model.get_attribution(x, targets)\n",
    "plt.imshow(gradcam.squeeze())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GuidedBackprop(XaiModel):\n",
    "    def __init__(self, model):\n",
    "        super(GuidedBackprop, self).__init__(model)\n",
    "        self.register_guided_hooks(self.model.convs)\n",
    "        \n",
    "    def register_guided_hooks(self, layers):\n",
    "        self.relu_hooks = []\n",
    "        self.reset_f_switches()\n",
    "        for layer in layers:\n",
    "            layer_name = type(layer).__name__\n",
    "            if layer_name.lower() == \"relu\":\n",
    "                def guided_forward(m, i, o):\n",
    "                    self.f_switches.append(o.data)\n",
    "                f_hook = XaiHook(layer, hook_fn=guided_forward)\n",
    "\n",
    "                def guided_backward(m, i, o):\n",
    "                    deconv_grad = o[0].clamp(min=0)  # o: backward input\n",
    "                    forward_output = self.f_switches.pop(-1)\n",
    "                    forward_mask = forward_output.ne(0.0).type_as(forward_output)\n",
    "                    grad_in = deconv_grad * forward_mask\n",
    "                    return (grad_in, )\n",
    "                \n",
    "                b_hook = XaiHook(layer, backward=True, hook_fn=guided_backward)\n",
    "                \n",
    "                self.relu_hooks.append(f_hook)\n",
    "                self.relu_hooks.append(b_hook)\n",
    "                \n",
    "        # register forward hooks\n",
    "        self._register(self.relu_hooks)\n",
    "        \n",
    "    def get_attribution(self, x, targets):\n",
    "        x.requires_grad_(True)\n",
    "        \n",
    "                \n",
    "        x_grad = x.grad.data\n",
    "        x.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(1, 1, kernel_size=(2, 2), stride=(1, 1))\n",
      "<class 'torch.nn.modules.conv.Conv2d'>\n",
      "forward_input\n",
      "tensor([[[[ 1.8423,  0.5189, -1.7119, -1.7014,  2.0194],\n",
      "          [-0.2686, -0.1307, -1.4374,  0.3908, -0.4340],\n",
      "          [ 0.3523, -0.0646,  0.7686,  0.9233, -0.5583],\n",
      "          [ 0.7796, -0.2617,  0.0317, -1.1724, -1.5069],\n",
      "          [-0.1481,  0.3715,  0.2273, -0.0647, -0.1479]]]])\n",
      "forward_output\n",
      "tensor([[[[-0.2046,  0.5226,  1.2223, -0.3424],\n",
      "          [ 0.5551,  1.3434,  0.9570,  0.5442],\n",
      "          [ 0.4624,  0.0151, -0.5609, -0.3226],\n",
      "          [ 0.5154,  0.5963,  0.9527,  1.1736]]]],\n",
      "       grad_fn=<ThnnConv2DBackward>)\n",
      "\n",
      "ReLU()\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "forward_input\n",
      "tensor([[[[-0.2046,  0.5226,  1.2223, -0.3424],\n",
      "          [ 0.5551,  1.3434,  0.9570,  0.5442],\n",
      "          [ 0.4624,  0.0151, -0.5609, -0.3226],\n",
      "          [ 0.5154,  0.5963,  0.9527,  1.1736]]]],\n",
      "       grad_fn=<ThnnConv2DBackward>)\n",
      "forward_output\n",
      "tensor([[[[0.0000, 0.5226, 1.2223, 0.0000],\n",
      "          [0.5551, 1.3434, 0.9570, 0.5442],\n",
      "          [0.4624, 0.0151, 0.0000, 0.0000],\n",
      "          [0.5154, 0.5963, 0.9527, 1.1736]]]], grad_fn=<ReluBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(5)\n",
    "x = torch.randn(1, 1, 5, 5)\n",
    "conv_layer = nn.Conv2d(1, 1, 2, 1)\n",
    "relu = nn.ReLU()\n",
    "def f_hook(m, i, o):\n",
    "    print(m)\n",
    "    print(type(m))\n",
    "    print(\"forward_input\")\n",
    "    for a in i:\n",
    "        print(a)\n",
    "    print(\"forward_output\")\n",
    "    print(o)\n",
    "    print()\n",
    "\n",
    "def b_hook(m, i, o):\n",
    "    print(m)\n",
    "    print(type(m))\n",
    "    print(\"grad_input\")\n",
    "    for a in i:\n",
    "        print(a)\n",
    "    print(\"grad_output\")\n",
    "    for a in o:\n",
    "        print(a)\n",
    "    print()\n",
    "\n",
    "conv_layer.register_forward_hook(f_hook)\n",
    "relu.register_forward_hook(f_hook)\n",
    "conv_layer.register_backward_hook(b_hook)\n",
    "relu.register_backward_hook(b_hook)\n",
    "o1 = conv_layer(x)\n",
    "o2 = relu(o1)\n",
    "o2_mask = (o2 == 0.0).byte()\n",
    "grad = torch.randn(1, 1, 3, 3)\n",
    "grad_mask = (grad < 0).byte()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1, 0, 0, 1],\n",
       "          [0, 0, 0, 0],\n",
       "          [0, 0, 1, 1],\n",
       "          [0, 0, 0, 0]]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o2_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 1, 0]]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.1333,  1.5883,  0.0059],\n",
       "          [ 0.6205,  0.8599,  0.9125],\n",
       "          [ 1.2162, -1.2440,  0.7325]]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU()\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "grad_input\n",
      "tensor([[[[ 0.0000, -0.0625, -0.0625,  0.0000],\n",
      "          [-0.0625, -0.0625, -0.0625, -0.0625],\n",
      "          [-0.0625, -0.0625,  0.0000,  0.0000],\n",
      "          [-0.0625, -0.0625, -0.0625, -0.0625]]]])\n",
      "grad_output\n",
      "tensor([[[[-0.0625, -0.0625, -0.0625, -0.0625],\n",
      "          [-0.0625, -0.0625, -0.0625, -0.0625],\n",
      "          [-0.0625, -0.0625, -0.0625, -0.0625],\n",
      "          [-0.0625, -0.0625, -0.0625, -0.0625]]]])\n",
      "\n",
      "Conv2d(1, 1, kernel_size=(2, 2), stride=(1, 1))\n",
      "<class 'torch.nn.modules.conv.Conv2d'>\n",
      "grad_input\n",
      "None\n",
      "tensor([[[[ 0.1859,  0.4519],\n",
      "          [-0.0822, -0.0112]]]])\n",
      "tensor([-0.7500])\n",
      "grad_output\n",
      "tensor([[[[ 0.0000, -0.0625, -0.0625,  0.0000],\n",
      "          [-0.0625, -0.0625, -0.0625, -0.0625],\n",
      "          [-0.0625, -0.0625,  0.0000,  0.0000],\n",
      "          [-0.0625, -0.0625, -0.0625, -0.0625]]]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(1 - o2.mean()).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
