{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "from collections import OrderedDict\n",
    "\n",
    "class XaiBase(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(XaiBase, self).__init__()\n",
    "        \"\"\"\n",
    "        need to define hook function at each method\n",
    "        - f_hook\n",
    "        - b_hook\n",
    "        \"\"\"\n",
    "        self.model = deepcopy(model)\n",
    "        self._reset_maps()\n",
    "        self.handlers = list()\n",
    "        \n",
    "        \n",
    "    def _reset_maps(self):\n",
    "        self.maps = OrderedDict()\n",
    "        \n",
    "    def _reset_handlers(self):\n",
    "        for handle in self.handlers:\n",
    "            handle.remove()\n",
    "        self.handlers = []\n",
    "                \n",
    "    def _register(self):\n",
    "        \"\"\"\n",
    "        need to define hook functions to use\n",
    "        \n",
    "        def f_hook(self, *x):\n",
    "            '''\n",
    "            m: module name\n",
    "            i: forward input\n",
    "            o: forward output\n",
    "            '''\n",
    "            m, i, o = x\n",
    "        \n",
    "        def b_hook(self, *x):\n",
    "            '''\n",
    "            m: module name\n",
    "            i: gradient input\n",
    "            o: gradient output\n",
    "            '''\n",
    "            m, i, o = x\n",
    "        \"\"\"\n",
    "        for layer in self.model.layers:\n",
    "            handle1 = layer.register_forward_hook(self.f_hook)\n",
    "            handle2 = layer.register_backward_hook(self.b_hook)\n",
    "            self.handlers.append(handle1)\n",
    "            self.handlers.append(handle2)    \n",
    "                \n",
    "    def save_maps(self, layer_name, x):\n",
    "        self.maps[layer_name] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test(XaiBase):\n",
    "    def __init__(self, model):\n",
    "        super(Test, self).__init__(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-19-383e3b6bdfbd>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-19-383e3b6bdfbd>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    class DeconvNet(XaiBase)\u001b[0m\n\u001b[1;37m                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class DeconvNet(XaiBase):\n",
    "    def __init__(self, model):\n",
    "        super(DeconvNet, self).__init__(model)\n",
    "        \n",
    "        self.deconvs = \n",
    "        \n",
    "        \n",
    "    def deconv_make_layers(self):\n",
    "        self.deconv_module_len = 0\n",
    "        layers = []\n",
    "        conv_end = [i for i, l in enumerate(self.model.convs) if str(l) == \"Reshape()\"][0]\n",
    "        # {0: 999, 3: 0}\n",
    "        conv_bias_pos = {}\n",
    "        conv_locs = [i for i, l in enumerate(model.layers[:conv_end]) if isinstance(l, nn.Conv2d)]\n",
    "        for idx, i in enumerate(conv_locs):\n",
    "            if idx == 0:\n",
    "                conv_bias_pos[i] = 999\n",
    "            else:\n",
    "                conv_bias_pos[i] = conv_locs[idx-1]\n",
    "\n",
    "        for idx, layer in enumerate(model.layers[:conv_end]):\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                temp_layer = nn.ConvTranspose2d(layer.out_channels,\n",
    "                                                layer.in_channels,\n",
    "                                                layer.kernel_size, \n",
    "                                                layer.stride, \n",
    "                                                layer.padding,\n",
    "                                                layer.output_padding,\n",
    "                                                layer.groups, \n",
    "                                                False,  # bias\n",
    "                                                layer.dilation,\n",
    "                                                layer.padding_mode)\n",
    "                temp_layer.weight.data = layer.weight.data\n",
    "                if conv_bias_pos[idx] < 999:\n",
    "                    temp_layer.bias = model.layers[:conv_end][conv_bias_pos[idx]].bias\n",
    "                layers.append(temp_layer)\n",
    "            elif isinstance(layer, nn.MaxPool2d):\n",
    "                temp_layer = nn.MaxUnpool2d(layer.kernel_size,\n",
    "                                            layer.stride,\n",
    "                                            layer.padding)\n",
    "                layers.append(temp_layer)\n",
    "                self.deconv_module_len += 1\n",
    "            else:\n",
    "                layers.append(layer)\n",
    "        layers = nn.Sequential(*reversed(layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeconvNet(nn.Module):\n",
    "    \"\"\"DeconvNet\"\"\"\n",
    "    def __init__(self, model):\n",
    "        super(DeconvNet, self).__init__()\n",
    "        # deconv\n",
    "        self.activation_func = model.activation_func\n",
    "        self.model_type = model.model_type\n",
    "        self.activation_type = model.activation_type\n",
    "        \n",
    "        self.layers = self.deconv_make_layers(model)\n",
    "        \n",
    "        self.activation_maps = OrderedDict()\n",
    "        \n",
    "    def deconv_make_layers(self, model):\n",
    "        self.deconv_module_len = 0\n",
    "        layers = []\n",
    "        conv_end = [i for i, l in enumerate(model.layers) if str(l) == \"Reshape()\"][0]\n",
    "        # {0: 999, 3: 0}\n",
    "        conv_bias_pos = {}\n",
    "        conv_locs = [i for i, l in enumerate(model.layers[:conv_end]) if isinstance(l, nn.Conv2d)]\n",
    "        for idx, i in enumerate(conv_locs):\n",
    "            if idx == 0:\n",
    "                conv_bias_pos[i] = 999\n",
    "            else:\n",
    "                conv_bias_pos[i] = conv_locs[idx-1]\n",
    "\n",
    "        for idx, layer in enumerate(model.layers[:conv_end]):\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                temp_layer = nn.ConvTranspose2d(layer.out_channels,\n",
    "                                                layer.in_channels,\n",
    "                                                layer.kernel_size, \n",
    "                                                layer.stride, \n",
    "                                                layer.padding,\n",
    "                                                layer.output_padding,\n",
    "                                                layer.groups, \n",
    "                                                False,  # bias\n",
    "                                                layer.dilation,\n",
    "                                                layer.padding_mode)\n",
    "                temp_layer.weight.data = layer.weight.data\n",
    "                if conv_bias_pos[idx] < 999:\n",
    "                    temp_layer.bias = model.layers[:conv_end][conv_bias_pos[idx]].bias\n",
    "                layers.append(temp_layer)\n",
    "            elif isinstance(layer, nn.MaxPool2d):\n",
    "                temp_layer = nn.MaxUnpool2d(layer.kernel_size,\n",
    "                                            layer.stride,\n",
    "                                            layer.padding)\n",
    "                layers.append(temp_layer)\n",
    "                self.deconv_module_len += 1\n",
    "            else:\n",
    "                layers.append(layer)\n",
    "        layers = nn.Sequential(*reversed(layers))\n",
    "        \n",
    "        deconv_locs = [i for i, l in enumerate(layers) if isinstance(l, nn.ConvTranspose2d)]\n",
    "        self.conv_end = conv_end\n",
    "        # {2: 2, 1: 5}\n",
    "        self.deconv_locs = {(len(deconv_locs) - j):i for j, i in enumerate(deconv_locs)}\n",
    "        return layers\n",
    "    \n",
    "    def save_activation_maps(self, layer, typ, idx, x):\n",
    "        if isinstance(layer, typ):\n",
    "            layer_name = f\"({idx}) {str(layer).split('(')[0]}(in:{layer.in_channels}, out:{layer.out_channels})\"\n",
    "            self.activation_maps[layer_name] = x\n",
    "    \n",
    "    def deconv(self, x, switches, deconv_layer_num=None, store=False):\n",
    "        \"\"\"\n",
    "        deconv_layer_num: \n",
    "            deconv from which module(m =  \"MaxPool > activation > Conv2d\") \n",
    "            numbering from the original cnn conv module(n = \"Conv2d > activation > MaxPool\")\n",
    "            ex) deconv_layers = [m1, m2, m3, m4, m5]\n",
    "                if deconv_layer_num = 4, will goes from m4 to m5\n",
    "                * cnn_layer = [n5, n4, n3, n2, n1] (n.T = m, n5 is the first layer of cnn)\n",
    "                \n",
    "        x: should match module input size\n",
    "        switches: from MNISTmodel forward method \"forward_switches\"\n",
    "        store: if True, save activation maps\n",
    "        \"\"\"\n",
    "        assert (deconv_layer_num <= self.deconv_module_len) or (deconv_layer_num==None), \\\n",
    "            \"`deconv_layer_num` should <= `self.deconv_module_len` or == None\"\n",
    "        if deconv_layer_num == None: deconv_layer_num = 1\n",
    "        deconvfrom = self.deconv_locs[deconv_layer_num]\n",
    "        deconvlayers = self.layers[-(deconvfrom+1):]\n",
    "        unpool_locs = {idx:(len(deconvlayers)-1 - idx) for idx, l in enumerate(deconvlayers) if isinstance(l, nn.MaxUnpool2d)}\n",
    "        \n",
    "        for idx, layer in enumerate(deconvlayers):\n",
    "            if isinstance(layer, nn.MaxUnpool2d):\n",
    "                x = layer(x, switches[unpool_locs[idx]])\n",
    "            else:\n",
    "                x = layer(x)\n",
    "                if store:\n",
    "                    self.save_activation_maps(layer, nn.ConvTranspose2d, idx, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"simonjisu\"\n",
    "# models/relavance\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(str(Path(__file__).absolute().parent.parent))\n",
    "from reshape import Reshape\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "from .layers import relLinear, relConv2d, relMaxPool2d, relReLU\n",
    "\n",
    "class LRP(nn.Module):\n",
    "    \"\"\"LRP\"\"\"\n",
    "    def __init__(self, model):\n",
    "        super(LRP, self).__init__()\n",
    "        # lrp\n",
    "        self.activation_func = model.activation_func\n",
    "        self.model_type = model.model_type\n",
    "        self.activation_type = model.activation_type\n",
    "        \n",
    "        self.layers = self.lrp_make_layers(model)\n",
    "        \n",
    "        self.activation_maps = OrderedDict()\n",
    "        \n",
    "    def lrp_make_layers(self, model):\n",
    "        layers = []\n",
    "        mapping_dict = {nn.Linear: relLinear, nn.Conv2d: relConv2d, nn.MaxPool2d: relMaxPool2d, \n",
    "                        nn.ReLU: relReLU}\n",
    "        for layer in model.layers:\n",
    "            if isinstance(layer, Reshape):\n",
    "                layers.append(layer)\n",
    "            else:\n",
    "                layers.append(mapping_dict[layer.__class__](layer))\n",
    "                \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        lrp method\n",
    "        must run forward first to save input and output at each layer\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def save_activation_maps(self, layer, typ, idx, x):\n",
    "        if isinstance(layer, typ):\n",
    "            layer_name = f\"({idx}) {str(layer).split('(')[0]}\"\n",
    "            self.activation_maps[layer_name] = x\n",
    "    \n",
    "    def get_attribution(self, x, target=None, store=False, use_rho=False):\n",
    "        \"\"\"\n",
    "        store: if True, save activation maps\n",
    "        \"\"\"\n",
    "        o = self.forward(x).detach()\n",
    "        r = o * torch.zeros_like(o).scatter(1, o.argmax(1, keepdim=True), 1)\n",
    "        for idx, layer in enumerate(self.layers[::-1]):\n",
    "            r = layer.relprop(r, use_rho)\n",
    "            if store:\n",
    "                self.save_activation_maps(layer, relConv2d, idx, r)\n",
    "        return r.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class VanillaGrad(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(VanillaGrad, self).__init__()\n",
    "        \n",
    "        # vanilla saliency\n",
    "        self.activation_func = model.activation_func\n",
    "        self.model_type = model.model_type\n",
    "        self.activation_type = model.activation_type\n",
    "\n",
    "        self.model = model.cpu()\n",
    "        self.model.eval()\n",
    "        \n",
    "    def get_attribution(self, x, target):\n",
    "        \"\"\"vanilla gradient\"\"\"\n",
    "        x.requires_grad_(requires_grad=True)\n",
    "        self.model.zero_grad()\n",
    "        o = self.model(x)\n",
    "        grad_outputs = torch.zeros_like(o).scatter(1, target.unsqueeze(1), 1).detach()\n",
    "        o.backward(gradient=grad_outputs)\n",
    "        x.requires_grad_(requires_grad=False)\n",
    "        \n",
    "        return x.grad.clone()\n",
    "\n",
    "\n",
    "class GradInput(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(GradInput, self).__init__()\n",
    "        \n",
    "        # vanilla saliency\n",
    "        self.activation_func = model.activation_func\n",
    "        self.model_type = model.model_type\n",
    "        self.activation_type = model.activation_type\n",
    "\n",
    "        self.model = model.cpu()\n",
    "        self.model.eval()\n",
    "        \n",
    "    def get_attribution(self, x, target):\n",
    "        \"\"\"vanilla gradient*input\"\"\"\n",
    "        x.requires_grad_(requires_grad=True)\n",
    "        self.model.zero_grad()\n",
    "        o = self.model(x)\n",
    "        grad_outputs = torch.zeros_like(o).scatter(1, target.unsqueeze(1), 1).detach()\n",
    "        o.backward(gradient=grad_outputs)\n",
    "        x.requires_grad_(requires_grad=False)\n",
    "        \n",
    "        return x.grad.clone() * x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
