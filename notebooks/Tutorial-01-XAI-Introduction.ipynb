{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../\")\n",
    "os.chdir(\"../\")\n",
    "from src.parserutils import get_parser\n",
    "from src.utils import Explorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [XAI Tutorial 01] MNIST / CIFAR10\n",
    "\n",
    "<span style=\"color:red\">**[Warning]**</span> This article is collection of some good articles that i read, so it contains a lot of references. Please visit them to learn more about XAI.\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [XAI](#1.-XAI)\n",
    "2. [Interpretability](#2.-Interpretability)\n",
    "    - What is Interpretability\n",
    "    - Why Interpretability\n",
    "    - Types of Interpretability\n",
    "3. [Deep Neural Network Interpretability](#3.-Deep-Neural-Network-Interpretability)\n",
    "    - Interpreting Models\n",
    "        - Representation Analysis\n",
    "        - Data Generation\n",
    "        - Example-based\n",
    "    - Interpreting Descions\n",
    "        - Example-based\n",
    "        - Attribution Methods\n",
    "4. [Evaluation of Attribution Methods](#4.-Evaluation-of-Attribution-Methods)\n",
    "    - Qualitative\n",
    "        - Coherence\n",
    "        - Class Sensitivity\n",
    "    - Quantitative\n",
    "        - Selectivity\n",
    "        - ROAR / KAR\n",
    "5. [Evaluation with MNIST & CIFAR10](#5.-Evaluation-with-MNIST-&-CIFAR10)\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* 1-1: [Wikipedia: Explainable_artificial_intelligence](https://en.wikipedia.org/wiki/Explainable_artificial_intelligence)\n",
    "* 2-1: [Explanation in Artificial Intelligence: Insights from the Social Sciences](https://arxiv.org/abs/1706.07269)\n",
    "* 2-2: [Explaining Explanations: An Overview of Interpretability of Machine Learning](https://arxiv.org/abs/1806.00069)\n",
    "* 3-1: [A Benchmark for Interpretability Methods in Deep Neural Networks](https://arxiv.org/abs/1806.10758)\n",
    "\n",
    "### Attribution Methods Papers:\n",
    "\n",
    "* [Not Just a Black Box: Learning Important Features Through Propagating Activation Differences](https://arxiv.org/abs/1605.01713)\n",
    "* [Striving for Simplicity: The All Convolutional Net](https://arxiv.org/abs/1412.6806)\n",
    "* [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/abs/1610.02391)\n",
    "\n",
    "## Useful Articles (To be updated)\n",
    "\n",
    "* [Molnar, Christoph. \"Interpretable machine learning. A Guide for Making Black Box Models Explainable\", 2019](https://christophm.github.io/interpretable-ml-book/)\n",
    "* [Explainable AI (Part I): Explanations and Opportunities](https://dainstudios.com/2019/10/24/explainable-ai-part-1/)\n",
    "* [Korean Review for References 1-2](https://simonjisu.github.io/paper/2019/09/18/introxai.html)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. XAI\n",
    "\n",
    "> Explainable AI (XAI) refers to methods and techniques in the application of artificial intelligence technology (AI) such that the results of the solution can be understood by human experts. It contrasts with the concept of the \"black box\" in machine learning where even their designers cannot explain why the AI arrived at a specific decision.\n",
    "> \n",
    "> From [References: 1-1](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Interpretability\n",
    "\n",
    "### What is Interpretability?\n",
    "\n",
    "> I use [Biran and Cotton’s](https://pdfs.semanticscholar.org/02e2/e79a77d8aabc1af1900ac80ceebac20abde4.pdf) definition of **interpretability** of a model as: the degree to which an observer can understand the cause of a decision.\n",
    "> \n",
    "> **From:** [References: 2-1](#References)\n",
    "\n",
    "> **The Goal of Interpretability**\n",
    ">\n",
    "> The goal of interpretability is to describe the internals of a system in a way that is understandable to humans. The success of this goal is tied to the cognition, knowledge, and biases of the user: for a system to be interpretable, it must produce descriptions that are simple enough for a person to understand using a vocabulary that is meaningful to the user.\n",
    "> \n",
    "> **What is an Explanation?**\n",
    ">\n",
    "> Philosophical texts show much debate over what constitutes an explanation. Of particular interest is what makes an explanation “good enough” or what really defines an explanation. Some say a good explanation depends on the question. This set of essays discusses the nature of explanation, theory, and the foundations of linguistics. Although for our work, the most important and interesting work is on “Why questions.” In particular, when you can phrase what you want to know from an algorithm as a why question, there is a natural qualitative representation of when you have answered said question–when you can no longer keep asking why. There are two why questions of interest; why and why-should. Similarly to the explainable planning literature, philosophers wonder about the why-shouldn’t and why-should questions, which can give the kinds of explainability requirements we want. \n",
    "> \n",
    "> **From:** [References: 2-2](#References)\n",
    "\n",
    "### Why Interpretability?\n",
    "\n",
    "> 1. Verify that model works as expected\n",
    ">     * Wrong decisions can be costly and dangerous: [Self-driving Uber kills Arizona woman in first fatal crash involving pedestrian](https://www.theguardian.com/technology/2018/mar/19/uber-self-driving-car-kills-woman-arizona-tempe) / Disease Misclassification\n",
    "> 2. Improve / Debug classifier\n",
    "> 3. Make new discoveries: \n",
    "    * Learn about the physical / biological / chemical mechanisms\n",
    "    * Learn about the human brain\n",
    "> 4. Right to explanation\n",
    ">     * US Equal Credit Opportunity Act\n",
    ">     * The European Union General Data Protection Regulation\n",
    ">     * France Digital Republic Act\n",
    "> \n",
    "> **From:** Taegyun Jeon(SIA, Founder and Chief Executive Office) in the lecture of AICollege (Not for Share)\n",
    "\n",
    "### Types of Interpretability in ML\n",
    "\n",
    "> * Ante-hoc Interpretability: Choose an interpretable model and train it.\n",
    ">     * Example: Decision Tree\n",
    ">     * Problem: Is the model expressive enough to predict the data?\n",
    "> * Post-hoc Interpretability: Choose a complex model and develop a special technique to interpret it.\n",
    ">     * Deep Neural Networks\n",
    ">     * How to interpret millions of parameters?\n",
    "> \n",
    "> **From:** The lecture of Taegyun Jeon(SIA, Founder and Chief Executive Office) in AICollege (Not for Share)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deep Neural Network Interpretability\n",
    "\n",
    "### Interpreting Models(Macroscopic)\n",
    "> Better understand internal representations\n",
    "> * \"Summarize\" DNN with a simpler model (e.g. decision tree) \n",
    "> * Find prototypical example of a category\n",
    "> * Find pattern maximizing activation of a neuron\n",
    ">\n",
    "> Types of Interpretability\n",
    "> * Representation Analysis\n",
    "> * Data Generation\n",
    "> * Example-based\n",
    "> \n",
    "> **From:** The lecture of Taegyun Jeon(SIA, Founder and Chief Executive Office) in AICollege (Not for Share)\n",
    "\n",
    "### Interpreting Descions(Microscopic)\n",
    "> Important for practical applications\n",
    "> - Why did DNN make this decision?\n",
    "> - Verify that model behaves as expected - Find evidence for decision\n",
    "> \n",
    "> Types of Interpretability\n",
    "> * Example-based\n",
    "> * Attribution Methods\n",
    "> \n",
    "> **From:** The lecture of Taegyun Jeon(SIA, Founder and Chief Executive Office) in AICollege (Not for Share)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation of Attribution Methods\n",
    "\n",
    "### Qualitative\n",
    "- Coherence\n",
    "- Class Sensitivity\n",
    "\n",
    "### Quantitative\n",
    "- Selectivity\n",
    "    > Process:\n",
    "    > 1. Evaluation with attribution method and get the attribution scores for the input datas. \n",
    "    > 2. Ranking the attribution scores and replace the input datas to 0 according to the ranked attribution scores with the percentage. Call them as \"perturbed datas\"\n",
    "    > 3. Ust the \"perturbed datas\" as a new input datas for model.\n",
    "    > 4. Repeat 1-3 process for a certain steps that you set. Also, record the metrics at each step.\n",
    "- ROAR / KAR\n",
    "    > Process:\n",
    "    > 1. Evaluation with attribution method and get the attribution scores for the input datas.\n",
    "    > 2. Ranking the attribution scores, after this using a small percentage(from 10% ~ 90%)\n",
    "    >     * [ROAR] replace the input datas to 0 according to the ranked attribution scores with the percentage.\n",
    "    >     * [KAR] begins with a datas filled with zeros, fill original datas according to the ranked attribution scores with the percentage.\n",
    "    >     * Call them as \"perturbed datas\"\n",
    "    > 3. Ust the \"perturbed datas\" as a new input datas for model. \n",
    "    > 4. Retrain the model and record the metrics.\n",
    "\n",
    "    > **What would happen without re-training?**\n",
    "    >\n",
    "    > The re-training is the most computationally expensive aspect of ROAR. One should question whether it is actually needed. We argue that re-training is needed because machine learning models **typically assume that the train and the test data comes from a similar distribution.**\n",
    "    >\n",
    "    > The replacement value c can only be considered uninformative if the model is trained to learn it as such. Without retraining, it is unclear whether degradation in performance is **due to the introduction of artifacts outside of the original training distribution** or **because we actually removed information**. This is made explicit in our experiment in Section 4.3.1, we show that without retraining the degradation is far higher than the modest decrease in performance observed with re-training. This suggests retraining has better controlled for artefacts introduced by the modification.\n",
    "    >\n",
    "    > **From**: [References: 3-1](#References)\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation with MNIST & CIFAR10\n",
    "\n",
    "1. Qualitative: See Attribution Maps with Mnist / Cifar10 Datas\n",
    "2. Quantitative: watch test accuracy in ROAR/KAR is decreasing / increasing.\n",
    "\n",
    "Builded a Baseline model which create a attribution map by random values from [0, 1).\n",
    "\n",
    "* all attributions are normalized to 0 to 255.\n",
    "\n",
    "### ROAR-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_roar_args = get_parser(data_type=\"mnist\", option=\"roar\", no_attention=True)\n",
    "mnist_roar_explorer = Explorer(mnist_roar_args)\n",
    "mnist_roar_explorer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_roar_explorer.show_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KAR-MNIST\n",
    "\n",
    "* percent of recover at 0 means the test accuracy for model's first trained result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_kar_args = get_parser(data_type=\"mnist\", option=\"kar\", no_attention=True)\n",
    "mnist_kar_explorer = Explorer(mnist_kar_args)\n",
    "mnist_kar_explorer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_kar_explorer.show_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROAR-CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_roar_rcd_args = get_parser(data_type=\"cifar10\", option=\"roar-rcd\", no_attention=True)\n",
    "cifar10_roar_rcd_explorer = Explorer(cifar10_roar_rcd_args)\n",
    "cifar10_roar_rcd_explorer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_roar_rcd_explorer.show_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KAR-CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_kar_rcd_args = get_parser(data_type=\"cifar10\", option=\"kar-rcd\", no_attention=True)\n",
    "cifar10_kar_rcd_explorer = Explorer(cifar10_kar_rcd_args)\n",
    "cifar10_kar_rcd_explorer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_kar_rcd_explorer.show_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "if you are in local jupyter notebook, open this notebook and change codes to change the option(you can't see codes through voila).\n",
    "\n",
    "- `data_type`: `\"mnist\"`, `\"cifar10\"`\n",
    "- `option`:\n",
    "    * Mnist Data: `\"roar\"`, `\"kar\"`\n",
    "    * Cifar10 Data: `\"roar\"`, `\"kar-rcd\"`, `\"roar-rcd\"`, `\"roar-rcd-fgm\"`, `\"roar-rcd-noabs\"`\n",
    "\n",
    "    > option-explanation:\n",
    "    >    * `rcd`: gray scale for all attribution methods(means that recuding the color dimension to 1)\n",
    "    >    * `fgm`: fill the masks with global mean of all datas instead of zeros.\n",
    "    >    * `noabs`: not to absolute attribution scores in some methods\n",
    "- `no_attention`: not to show the attention models\n",
    "\n",
    "Code:\n",
    "\n",
    "```python\n",
    "args = get_parser(data_type=\"cifar10\", option=\"roar-rcd\", no_attention=False)\n",
    "explorer = Explorer(args)\n",
    "explorer.show()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
