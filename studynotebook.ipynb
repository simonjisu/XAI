{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchxai.base import XaiBase, XaiHook, XaiModel\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlimpseSensor(nn.Module):\n",
    "    \"\"\"Glimpse Sensor\"\"\"\n",
    "    def __init__(self, g, k, s):\n",
    "        \"\"\"\n",
    "        retina and location encoding\n",
    "        ----\n",
    "        The retina encoding `ρ(x, l)` extracts `k` square \n",
    "        patches centered at location `l`, with the first patch \n",
    "        being `g_w` × `g_w` pixels in size, and each successive \n",
    "        patch having twice the width of the previous. \n",
    "        The k patches are then all resized to `g_w × g_w` \n",
    "        and concatenated. \n",
    "        Glimpse locations `l` were encoded \n",
    "        as real-valued (x, y) coordinates with \n",
    "        (0, 0) being the center of the image x and \n",
    "        (−1, −1) being the top left corner of x.\n",
    "        \n",
    "        args:\n",
    "        - g: size of first square patches\n",
    "        - k: number of patches\n",
    "        - s: scaling factor to control patches\n",
    "        \"\"\"\n",
    "        super(GlimpseSensor, self).__init__()\n",
    "        self.g = g\n",
    "        self.k = k\n",
    "        selk.s = s\n",
    "    \n",
    "    def extract_patch(self, x, l):\n",
    "        B, C, H, W = x.size()\n",
    "        \n",
    "    \n",
    "    def encode_coordinates(self, l):\n",
    "        \"\"\"\n",
    "        encode coordinates to (-1, 1) range\n",
    "        - center: (0, 0)\n",
    "        - topleft: (-1, -1)\n",
    "        \"\"\"\n",
    "        \n",
    "        return \n",
    "        \n",
    "    \n",
    "    def forward(self, x, l):\n",
    "        \"\"\"\n",
    "        args:\n",
    "        - x: current time step input data, (B, C, H, W)\n",
    "        - l: previous time step location, (B, 2)\n",
    "        \n",
    "        returns:\n",
    "        - rho: retina-like representation\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord = torch.FloatTensor([[19, 8],[20, 20]])\n",
    "H = 28\n",
    "coords_normed = (0.5 * ((coord + 1.0)*H)).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = coords_normed[:, 0] - (5 // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([278, 292])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(28 - 0.5) / 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0357, 0.0714, 0.1071, 0.1429, 0.1786, 0.2143, 0.2500, 0.2857,\n",
       "        0.3214, 0.3571, 0.3929, 0.4286, 0.4643, 0.5000, 0.5357, 0.5714, 0.6071,\n",
       "        0.6429, 0.6786, 0.7143, 0.7500, 0.7857, 0.8214, 0.8571, 0.8929, 0.9286,\n",
       "        0.9643], dtype=torch.float64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(28, dtype=float) / 28.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AnR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \"\"\"AttentionHead\"\"\"\n",
    "    def __init__(self, in_c, n_head):\n",
    "        super(AttentionHead, self).__init__()\n",
    "        \"\"\"\n",
    "        3.2 Attention Head\n",
    "        \n",
    "        args:\n",
    "        - in_c: C\n",
    "        - n_head: K\n",
    "        \"\"\"\n",
    "        self.n_head = n_head\n",
    "        self.conv = nn.Conv2d(in_c, n_head, kernel_size=3, padding=1, bias=False)\n",
    "        self.diag = (1 - torch.eye(n_head, n_head))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        args:\n",
    "        - x: feature activations, (B, C, H, W)\n",
    "        \n",
    "        returns:\n",
    "        - Tensor, K-attention masks(softmax by channel-wise)  (B, K, H, W)\n",
    "        \"\"\"\n",
    "        B = x.size(0)\n",
    "        conv_heads = self.conv(x)  # (B, C, H, W) > (B, K, H, W)\n",
    "        masks = torch.softmax(conv_heads, dim=1)  # K-attention masks\n",
    "        self.masks_score = masks.view(B, self.n_head, -1)\n",
    "        return masks  # (B, K, H, W)\n",
    "    \n",
    "    def reg_loss(self):\n",
    "        \"\"\"\n",
    "        calculate reg_loss\n",
    "        \"\"\"\n",
    "        # (B, K, H*W) x (B, H*W, K) > (B, K, K)\n",
    "        reg_loss = self.diag.to(self.masks.device) * torch.bmm(self.masks_score, self.masks_score.transpose(1, 2))\n",
    "        return reg_loss\n",
    "\n",
    "class AttentionOut(nn.Module):\n",
    "    \"\"\"AttentionOut\"\"\"\n",
    "    def __init__(self, in_c, n_head, n_label=1, gate=False):\n",
    "        super(AttentionOut, self).__init__()\n",
    "        \"\"\"\n",
    "        3.3 Output head / 3.4 Layered attention gates\n",
    "        \n",
    "        args:\n",
    "        - in_c: C\n",
    "        - n_head: K\n",
    "        - n_label: L\n",
    "        - gate: if gate is `True`, returns attention gates\n",
    "        \"\"\"\n",
    "        self.n_head = n_head\n",
    "        self.n_label = n_label\n",
    "        self.gate = gate\n",
    "        if gate:\n",
    "            assert self.n_label == 1, \"Gate must set `n_label = 1`\"\n",
    "        self.conv = nn.Conv2d(in_c, n_head*n_label, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x, masks):\n",
    "        \"\"\"\n",
    "        args:\n",
    "        - x: feature activations, (B, C, H, W)\n",
    "        - masks: masks from `AttentionHead`, (B, K, H, W)\n",
    "        \n",
    "        returns:\n",
    "        - scores: when `self.gate=False`, (B, K, L)\n",
    "        - gates: when `self.gate=True`, (B, K, 1)\n",
    "        \"\"\"\n",
    "        B = x.size(0)\n",
    "        conv_outputs = self.conv(x)  # (B, C, H, W) > (B, K*L, H, W)\n",
    "        outputs = conv_outputs.view(B, self.n_head, self.n_label, -1)  # (B, K, L, H*W)\n",
    "        # (B, K, L, H*W) * (B, K, 1, H*W) > (B, K, L)\n",
    "        scores = (outputs * masks.view(B, self.n_head, -1).unsqueeze(2)).sum(-1)\n",
    "        if not self.gate:\n",
    "            return scores\n",
    "        else:\n",
    "            # L = 1, returns Tensor (B, K, 1)\n",
    "            gates = torch.softmax(torch.tanh(scores), dim=1)\n",
    "            return gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionModule(nn.Module):\n",
    "    \"\"\"AttentionModule\"\"\"\n",
    "    def __init__(self, in_c, n_head, n_label, reg_weight=0.0):\n",
    "        \"\"\"\n",
    "        calculate outputs of attention module\n",
    "        \n",
    "        args:\n",
    "        - in_c: the number of input channels\n",
    "        - n_head: the attention width, the number of layers using the attention mechanism\n",
    "        - n_label: the number of class labels\n",
    "        \"\"\"\n",
    "        super(AttentionModule, self).__init__()\n",
    "        self.reg_weight = reg_weight\n",
    "        self.attn_heads = AttentionHead(in_c, n_head)\n",
    "        self.output_heads = AttentionOut(in_c, n_head, n_label=n_label, gate=False)\n",
    "        self.attn_gates = AttentionOut(in_c, n_head, gate=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        args:\n",
    "        - x: feature activations, (B, C, H, W)\n",
    "        \n",
    "        returns:\n",
    "        - outputs_vectors: predict vectors which applied the most meaningful attention head, (B, L)\n",
    "        \n",
    "        \"\"\"\n",
    "        masks = self.attn_heads(x)  # (B, K, H, W)  softmax: dim=1\n",
    "        outputs = self.output_heads(x, masks)  # (B, K, L)\n",
    "        gates = self.attn_gates(x, masks)  # (B, K, 1)  softmax: dim=1\n",
    "        outputs_vectors = (outputs * gates).sum(1)  # (B, L)\n",
    "        return outputs_vectors\n",
    "        \n",
    "    def reg_loss(self):\n",
    "        return self.attn_heads.reg_loss() * self.reg_weight\n",
    "\n",
    "\n",
    "class GlobalAttentionGate(nn.Module):\n",
    "    \"\"\"GlobalAttentionGate\"\"\"\n",
    "    def __init__(self, in_c, n_hypothesis, gate_fn=\"softmax\"):\n",
    "        \"\"\"\n",
    "        args:\n",
    "        - in_c: the number of input channels\n",
    "        - n_hypothesis: the number of total hypothesis, including original output vector and attention outputs(count as: N)\n",
    "        \"\"\"\n",
    "        super(GlobalAttentionGate, self).__init__()\n",
    "        self.n_hypothesis = n_hypothesis\n",
    "        self.gate_fn = gate_fn\n",
    "        self.gate_layer = nn.Linear(in_c, n_gates, bias=False)\n",
    "    \n",
    "    def cal_global_gates(self, x):\n",
    "        x = torch.flatten(x, 1)  # (B, C*H*W) > (B, N+1)\n",
    "        c = torch.tanh(self.gate_layer(x))\n",
    "        if self.gate_fn == \"softmax\":\n",
    "            global_gates = torch.softmax(c, dim=1)\n",
    "        elif self.gate_fn == \"sigmoid\":\n",
    "            global_gates = torch.sigmoid(c)\n",
    "        return global_gates.unsqueeze(-1)  # (B, N+1, 1)\n",
    "    \n",
    "    def forward(self, x, hypothesis):\n",
    "        \"\"\"\n",
    "        args:\n",
    "        - x: last feature activations, (B, C, H, W)\n",
    "        - hypothesis: all hypothesis, list type contains N+1 of (B, 1, L) size Tensor\n",
    "        \n",
    "        returns:\n",
    "        - global_gates: (B, N+1)\n",
    "        \"\"\"\n",
    "        global_gates = self.cal_global_gates(x)  # (B, N+1, 1)\n",
    "        outputs = torch.cat(hypothesis, dim=1)  #(B, N+1, L)\n",
    "        outputs = torch.log_softmax(outputs, dim=2)  # calculate log probs\n",
    "        outputs_net = (outputs * global_gates).sum(1)  # (B, L)\n",
    "        return outputs_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 3, 28, 28)\n",
    "in_c = 3\n",
    "n_head = 5\n",
    "n_label = 10\n",
    "attn_heads = AttentionHead(in_c, n_head)\n",
    "output_heads = AttentionOut(in_c, n_head, n_label)\n",
    "attn_gates = AttentionOut(in_c, n_head, gate=True)\n",
    "attn_m = AttentionModule(in_c, n_head, n_label)\n",
    "n_gates = 4\n",
    "attn_gg = GlobalAttentionGate(in_c, n_gates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 28, 28]) torch.Size([1, 5, 10]) torch.Size([1, 5, 1])\n"
     ]
    }
   ],
   "source": [
    "H = attn_heads(x)\n",
    "O = output_heads(x, H)\n",
    "G = attn_gates(x, H)\n",
    "\n",
    "print(H.size(), O.size(), G.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-9ca03a85c661>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mOV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_m\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mattn_gg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'x'"
     ]
    }
   ],
   "source": [
    "OV = attn_m(x)\n",
    "attn_gg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 14, 14])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.functional.avg_pool2d(H, 2, 2).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
